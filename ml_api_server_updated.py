#!/usr/bin/env python3
"""
EC2 ML ì„œë²„ - FastAPI ê¸°ë°˜

WhisperX, í™”ì ë¶„ë¦¬, ê°ì • ë¶„ì„ ë“±ì„ HTTP APIë¡œ ì œê³µí•˜ëŠ” ì„œë²„
ECS FastAPI ë°±ì—”ë“œë¡œë¶€í„° ìš”ì²­ì„ ë°›ì•„ JSONìœ¼ë¡œ ê²°ê³¼ ë°˜í™˜
"""

import asyncio
import os
import sys
import logging
import tempfile
import uuid
import requests
from pathlib import Path
from typing import Dict, Any, Optional, List
from datetime import datetime

import boto3
import uvicorn
from fastapi import FastAPI, HTTPException, BackgroundTasks, Query
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

try:
    from src.services.callback_pipeline import process_video_with_fastapi
    from src.api import AnalysisConfig
    from src.utils.logger import get_logger
except ImportError as e:
    print(f"Warning: Import failed: {e}")
    # Mock í´ë˜ìŠ¤ë“¤
    class AnalysisConfig:
        def __init__(self, **kwargs):
            for k, v in kwargs.items():
                setattr(self, k, v)
    
    def get_logger(name):
        return logging.getLogger(name)

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="ECG Model Server",
    description="EC2 ML ì„œë²„ - ë¹„ë””ì˜¤ ì˜¤ë””ì˜¤ ë¶„ì„ API",
    version="1.0.0"
)

# CORS ì„¤ì • - ECS ë°±ì—”ë“œì™€ì˜ í†µì‹ ì„ ìœ„í•´
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ìš´ì˜ í™˜ê²½ì—ì„œëŠ” íŠ¹ì • ë„ë©”ì¸ìœ¼ë¡œ ì œí•œ
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# AWS S3 ë° Backend ì„¤ì •
s3_client = boto3.client('s3')
S3_BUCKET = "ecg-project-pipeline-dev-video-storage-np9digv7"
BACKEND_URL = "http://ecg-project-pipeline-dev-alb-1703405864.us-east-1.elb.amazonaws.com"

# In-memory job tracking
jobs: Dict[str, Dict[str, Any]] = {}

# ë¡œê±° ì„¤ì •
logger = get_logger(__name__)

# =============================================================================
# Pydantic ëª¨ë¸ë“¤ (íƒ€ì… ì–´ë…¸í…Œì´ì…˜ í¬í•¨)
# =============================================================================

class ProcessRequest(BaseModel):
    """ë¹„ë””ì˜¤ ë¶„ì„ ìš”ì²­ - ML_API.md ëª…ì„¸ ì¤€ìˆ˜"""
    job_id: str = Field(..., description="ì‘ì—… ID")
    video_url: str = Field(..., description="S3 ë¹„ë””ì˜¤ URL")
    enable_gpu: Optional[bool] = Field(True, description="GPU ì‚¬ìš© ì—¬ë¶€")
    emotion_detection: Optional[bool] = Field(True, description="ê°ì • ë¶„ì„ ì—¬ë¶€")
    language: Optional[str] = Field("auto", description="ì–¸ì–´ ì„¤ì •")
    max_workers: Optional[int] = Field(4, description="ìµœëŒ€ ì›Œì»¤ ìˆ˜")

class ProcessResponse(BaseModel):
    """ë¹„ë””ì˜¤ ë¶„ì„ ì‘ë‹µ - ML_API.md ëª…ì„¸ ì¤€ìˆ˜"""
    job_id: str = Field(..., description="ì‘ì—… ID")
    status: str = Field("processing", description="ì‘ì—… ìƒíƒœ")

class JobStatusResponse(BaseModel):
    """ì‘ì—… ìƒíƒœ ì‘ë‹µ - ML_API.md ëª…ì„¸ ì¤€ìˆ˜"""
    job_id: str = Field(..., description="ì‘ì—… ID")
    status: str = Field(..., description="ì‘ì—… ìƒíƒœ: processing, completed, failed")
    progress: Optional[int] = Field(None, description="ì§„í–‰ë¥  (0-100)")
    result: Optional[Dict[str, Any]] = Field(None, description="ë¶„ì„ ê²°ê³¼ (ì™„ë£Œì‹œ)")
    error: Optional[str] = Field(None, description="ì—ëŸ¬ ë©”ì‹œì§€ (ì‹¤íŒ¨ì‹œ)")

class MLResultRequest(BaseModel):
    """ML ì„œë²„ ê²°ê³¼ ìš”ì²­ (ë°±ì—”ë“œ ì½œë°±ìš©) - ML_API.md ëª…ì„¸ ì¤€ìˆ˜"""
    job_id: str = Field(..., description="ì‘ì—… ID")
    result: Dict[str, Any] = Field(..., description="Whisper ì›ë³¸ JSON ê²°ê³¼")

class HealthResponse(BaseModel):
    """í—¬ìŠ¤ ì²´í¬ ì‘ë‹µ"""
    status: str = Field(..., description="ì„œë²„ ìƒíƒœ")
    service: str = Field(..., description="ì„œë¹„ìŠ¤ ì´ë¦„")
    version: str = Field(..., description="ë²„ì „")
    gpu_available: Optional[bool] = Field(None, description="GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€")
    gpu_count: Optional[int] = Field(None, description="GPU ê°œìˆ˜")
    timestamp: str = Field(..., description="ì‘ë‹µ ì‹œê°„")

# =============================================================================
# API ì—”ë“œí¬ì¸íŠ¸ë“¤
# =============================================================================

@app.post("/api/upload-video/process-video", response_model=ProcessResponse)
async def process_video(
    request: ProcessRequest,
    background_tasks: BackgroundTasks = None
):
    """
    ë¹„ë™ê¸° ë¹„ë””ì˜¤ ë¶„ì„ ìš”ì²­ - ML_API.md ëª…ì„¸ ì¤€ìˆ˜
    
    Backendê°€ í˜¸ì¶œí•˜ëŠ” API:
    POST /api/upload-video/process-video
    {
        "job_id": "123456",
        "video_url": "https://bucket.s3.amazonaws.com/file.mp4"
    }
    """
    try:
        # ì‘ì—… ìƒíƒœ ì´ˆê¸°í™”
        jobs[request.job_id] = {
            "status": "processing",
            "video_url": request.video_url,
            "progress": 0,
            "created_at": datetime.now().isoformat()
        }
        
        logger.info(f"ë¹„ë™ê¸° ë¶„ì„ ìš”ì²­ ì‹œì‘ - job_id: {request.job_id}, video_url: {request.video_url}")
        
        # ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì‹œì‘
        if background_tasks:
            background_tasks.add_task(process_video_async, request.job_id, request.video_url)
        else:
            # ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì§ì ‘ ì‹¤í–‰
            asyncio.create_task(process_video_async(request.job_id, request.video_url))
        
        return ProcessResponse(
            job_id=request.job_id,
            status="processing"
        )
        
    except Exception as e:
        logger.error(f"ë¶„ì„ ìš”ì²­ ì‹¤íŒ¨ - Error: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"ë¶„ì„ ìš”ì²­ ì‹¤íŒ¨: {str(e)}"
        )

@app.get("/api/upload-video/status/{job_id}", response_model=JobStatusResponse)
async def get_job_status(job_id: str):
    """ì‘ì—… ìƒíƒœ ì¡°íšŒ - ML_API.md ëª…ì„¸ ì¤€ìˆ˜"""
    if job_id not in jobs:
        raise HTTPException(status_code=404, detail="ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    job = jobs[job_id]
    return JobStatusResponse(
        job_id=job_id,
        status=job["status"],
        progress=job.get("progress"),
        result=job.get("result"),
        error=job.get("error")
    )

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """ML ì„œë²„ í—¬ìŠ¤ ì²´í¬ - Backend í˜¸í™˜"""
    
    # GPU ì •ë³´ í™•ì¸
    gpu_available = False
    gpu_count = 0
    
    try:
        import torch
        if torch.cuda.is_available():
            gpu_available = True
            gpu_count = torch.cuda.device_count()
    except ImportError:
        pass
    
    return HealthResponse(
        status="healthy",
        service="ecg-ml-server",
        version="1.0.0",
        gpu_available=gpu_available,
        gpu_count=gpu_count,
        timestamp=datetime.now().isoformat()
    )

@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "service": "ECG Audio Analyzer ML API",
        "version": "1.0.0", 
        "description": "EC2 ML ì„œë²„ - ë¹„ë””ì˜¤ ì˜¤ë””ì˜¤ ë¶„ì„ API",
        "endpoints": {
            "process-video": "POST /api/upload-video/process-video - ë¹„ë™ê¸° ë¹„ë””ì˜¤ ë¶„ì„",
            "job-status": "GET /api/upload-video/status/{job_id} - ì‘ì—… ìƒíƒœ ì¡°íšŒ",
            "health": "GET /health - í—¬ìŠ¤ ì²´í¬",
            "docs": "GET /docs - API ë¬¸ì„œ"
        },
        "backend_integration": {
            "s3_bucket": S3_BUCKET,
            "backend_url": BACKEND_URL,
            "active_jobs": len([j for j in jobs.values() if j["status"] == "processing"])
        }
    }

@app.post("/test")
async def test_endpoint():
    """í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "status": "ok",
        "message": "ML API ì„œë²„ê°€ ì •ìƒ ì‘ë™ì¤‘ì…ë‹ˆë‹¤",
        "timestamp": datetime.now().isoformat(),
        "server_info": {
            "python_version": sys.version,
            "working_directory": str(Path.cwd()),
            "environment": os.environ.get("ENVIRONMENT", "development")
        }
    }

# =============================================================================
# ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ í•¨ìˆ˜ë“¤
# =============================================================================

async def process_video_async(job_id: str, video_url: str):
    """
    ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë¹„ë””ì˜¤ ë¶„ì„ ë° Backendë¡œ ê²°ê³¼ ì „ì†¡
    """
    try:
        logger.info(f"ë°±ê·¸ë¼ìš´ë“œ ë¶„ì„ ì‹œì‘ - job_id: {job_id}, video_url: {video_url}")
        
        # ì§„í–‰ìƒí™© ì—…ë°ì´íŠ¸
        jobs[job_id]["progress"] = 10
        
        # S3 URLì—ì„œ íŒŒì¼ í‚¤ ì¶”ì¶œ
        if video_url.startswith("https://"):
            # URLì—ì„œ íŒŒì¼ í‚¤ ì¶”ì¶œ: https://bucket.s3.amazonaws.com/path/file.mp4 -> path/file.mp4
            file_key = "/".join(video_url.split("/")[4:])
        else:
            file_key = video_url  # ì´ë¯¸ íŒŒì¼ í‚¤ì¸ ê²½ìš°
            
        # ì§„í–‰ìƒí™© ì—…ë°ì´íŠ¸
        jobs[job_id]["progress"] = 20
        
        # ì„ì‹œ íŒŒì¼ì— S3ì—ì„œ ë¹„ë””ì˜¤ ë‹¤ìš´ë¡œë“œ
        with tempfile.NamedTemporaryFile(suffix='.mp4', delete=False) as temp_file:
            temp_path = temp_file.name
            
            try:
                s3_client.download_file(S3_BUCKET, file_key, temp_path)
                logger.info(f"S3ì—ì„œ ë¹„ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {file_key}")
                jobs[job_id]["progress"] = 40
            except Exception as s3_error:
                logger.warning(f"S3 ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨, ëª¨í¬ ë°ì´í„° ìƒì„±: {s3_error}")
                # S3 ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ì‹œ ë”ë¯¸ ë°ì´í„° ìƒì„±
                with open(temp_path, 'wb') as f:
                    f.write(b"dummy video data for testing")
                jobs[job_id]["progress"] = 30
            
            try:
                # ì§„í–‰ìƒí™© ì—…ë°ì´íŠ¸
                jobs[job_id]["progress"] = 60
                
                # ì‹¤ì œ ë¶„ì„ ì‹¤í–‰ ì‹œë„
                result = await run_actual_analysis(temp_path, file_key, job_id)
                
                # ğŸ” ê²°ê³¼ ê²€ì¦ ë° ë¡œê¹… ê°•í™”
                logger.info(f"ë¶„ì„ ê²°ê³¼ íƒ€ì…: {type(result)}")
                logger.info(f"ë¶„ì„ ê²°ê³¼ ë‚´ìš© (ì²˜ìŒ 500ì): {str(result)[:500]}")
                
                if result is None:
                    logger.error("ë¶„ì„ ê²°ê³¼ê°€ Noneì…ë‹ˆë‹¤!")
                    raise Exception("ë¶„ì„ ê²°ê³¼ê°€ Noneì…ë‹ˆë‹¤")
                
                # ì§„í–‰ìƒí™© ì—…ë°ì´íŠ¸
                jobs[job_id]["progress"] = 90
                
                # ê²°ê³¼ë¥¼ Whisper í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                if hasattr(result, 'model_dump'):
                    # Pydantic ëª¨ë¸ì¸ ê²½ìš°
                    whisper_result = result.model_dump()
                    logger.info("Pydantic ëª¨ë¸ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜")
                elif hasattr(result, '__dict__'):
                    # ì¼ë°˜ ê°ì²´ì¸ ê²½ìš°
                    whisper_result = result.__dict__
                    logger.info("ê°ì²´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜")
                else:
                    # ì´ë¯¸ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš°
                    whisper_result = result
                    logger.info("ì´ë¯¸ ë”•ì…”ë„ˆë¦¬ í˜•íƒœ")
                
                logger.info(f"ë³€í™˜ëœ ê²°ê³¼ í‚¤: {list(whisper_result.keys()) if isinstance(whisper_result, dict) else 'Not a dict'}")
                
                # ì„±ê³µì ì¸ ë¶„ì„ ê²°ê³¼ - ML_API.md ëª…ì„¸ì— ë§ê²Œ ë‹¨ìˆœí™”
                backend_result = {
                    "job_id": job_id,
                    "result": whisper_result  # ë³€í™˜ëœ ê²°ê³¼
                }
                
            except Exception as analysis_error:
                logger.error(f"ë¶„ì„ ì‹¤íŒ¨: {analysis_error}")
                jobs[job_id]["progress"] = 100
                
                # ë¶„ì„ ì‹¤íŒ¨ì‹œ ëª¨í¬ ê²°ê³¼ ìƒì„±
                backend_result = {
                    "job_id": job_id,
                    "result": create_mock_whisper_result()
                }
            
            finally:
                # ì„ì‹œ íŒŒì¼ ì •ë¦¬
                try:
                    os.unlink(temp_path)
                except:
                    pass
            
            # ì‘ì—… ìƒíƒœ ì—…ë°ì´íŠ¸
            jobs[job_id].update({
                "status": "completed",
                "progress": 100,
                "result": backend_result,
                "video_url": video_url,
                "completed_at": datetime.now().isoformat()
            })
            
            # Backendë¡œ ê²°ê³¼ ì „ì†¡
            await send_result_to_backend(backend_result)
            
            logger.info(f"ë¶„ì„ ì™„ë£Œ - job_id: {job_id}")
            
    except Exception as e:
        jobs[job_id].update({
            "status": "failed",
            "progress": 100,
            "error": str(e),
            "video_url": video_url,
            "failed_at": datetime.now().isoformat()
        })
        logger.error(f"ë¶„ì„ ì‹¤íŒ¨ - job_id: {job_id}, error: {str(e)}")

async def run_actual_analysis(temp_path: str, file_key: str, job_id: str):
    """ì‹¤ì œ WhisperX ë¶„ì„ ì‹¤í–‰"""
    try:
        # ë¶„ì„ ì„¤ì • ìƒì„±
        config = AnalysisConfig(
            enable_gpu=True,
            emotion_detection=True,
            language="auto",
            max_workers=4
        )
        
        # WhisperX íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œë„
        from src.services.callback_pipeline import CallbackPipelineManager
        
        pipeline = CallbackPipelineManager(config, fastapi_client=None)
        
        # ì§„í–‰ìƒí™© ì½œë°± í•¨ìˆ˜
        async def progress_callback(stage: str, progress: float, message: str = None):
            # 60-85% ë²”ìœ„ì—ì„œ ì§„í–‰ìƒí™© ì—…ë°ì´íŠ¸
            actual_progress = 60 + int(progress * 25)
            jobs[job_id]["progress"] = min(85, actual_progress)
            logger.info(f"Progress: {stage} - {progress:.1%} - {message or ''}")
        
        result = await pipeline._run_pipeline_with_progress(
            input_source=temp_path,
            progress_callback=progress_callback,
            output_path=None
        )
        
        return result
        
    except ImportError as e:
        logger.warning(f"WhisperX ëª¨ë“ˆ ë¡œë”© ì‹¤íŒ¨: {e}")
        raise Exception("WhisperX ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    except Exception as e:
        logger.error(f"ë¶„ì„ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        raise

def create_mock_whisper_result():
    """ML_API.md ëª…ì„¸ì— ë§ëŠ” ëª¨í¬ Whisper ê²°ê³¼ ìƒì„±"""
    return {
        "text": "ì•ˆë…•í•˜ì„¸ìš”, ì´ê²ƒì€ ëª¨í¬ ë°ì´í„°ì…ë‹ˆë‹¤. ì‹¤ì œ ë¶„ì„ì€ WhisperXë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤.",
        "segments": [
            {
                "start": 0.0,
                "end": 3.0,
                "text": "ì•ˆë…•í•˜ì„¸ìš”, ì´ê²ƒì€ ëª¨í¬ ë°ì´í„°ì…ë‹ˆë‹¤.",
                "speaker": "SPEAKER_00"
            },
            {
                "start": 3.0,
                "end": 6.0,
                "text": "ì‹¤ì œ ë¶„ì„ì€ WhisperXë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤.",
                "speaker": "SPEAKER_01"
            }
        ],
        "language": "ko",
        "duration": 6.0,
        "speakers_count": 2
    }


async def send_result_to_backend(result: Dict[str, Any]):
    """Backendë¡œ ê²°ê³¼ ì „ì†¡ - ML_API.md ëª…ì„¸ ì¤€ìˆ˜"""
    try:
        response = requests.post(
            f"{BACKEND_URL}/api/upload-video/result",  # ë‹¨ìˆ˜í˜•ìœ¼ë¡œ ìˆ˜ì •
            json=result,
            timeout=30,
            headers={"Content-Type": "application/json"}
        )
        
        if response.status_code == 200:
            logger.info(f"Backendë¡œ ê²°ê³¼ ì „ì†¡ ì„±ê³µ - job_id: {result['job_id']}")
        else:
            logger.warning(f"Backend ì‘ë‹µ ìƒíƒœ: {response.status_code}, ë‚´ìš©: {response.text}")
            
    except Exception as backend_error:
        logger.error(f"Backend ê²°ê³¼ ì „ì†¡ ì‹¤íŒ¨: {backend_error}")


# =============================================================================
# ë ˆê±°ì‹œ ì—”ë“œí¬ì¸íŠ¸ (í˜¸í™˜ì„± ìœ ì§€)
# =============================================================================

@app.post("/request-process")
async def request_process_legacy(
    fileKey: str = Query(..., description="S3 íŒŒì¼ í‚¤"),
    background_tasks: BackgroundTasks = None
):
    """ë ˆê±°ì‹œ ì—”ë“œí¬ì¸íŠ¸ (í˜¸í™˜ì„± ìœ ì§€)"""
    # ê¸°ì¡´ í˜•ì‹ì„ ìƒˆ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    job_id = str(uuid.uuid4())
    # S3 URL í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ê°„ë‹¨í•œ ì¶”ì •)
    video_url = f"https://{S3_BUCKET}.s3.amazonaws.com/{fileKey}"
    
    request = ProcessRequest(
        job_id=job_id,
        video_url=video_url
    )
    
    return await process_video(request, background_tasks)

# =============================================================================
# ì„œë²„ ì‹¤í–‰ ì½”ë“œ
# =============================================================================

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="ECG Audio Analyzer ML API Server")
    parser.add_argument("--host", default="0.0.0.0", help="ë°”ì¸ë“œ í˜¸ìŠ¤íŠ¸")
    parser.add_argument("--port", type=int, default=8080, help="ë°”ì¸ë“œ í¬íŠ¸")
    parser.add_argument("--workers", type=int, default=1, help="ì›Œì»¤ ìˆ˜")
    parser.add_argument("--log-level", default="info", choices=["debug", "info", "warning", "error"])
    
    args = parser.parse_args()
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=getattr(logging, args.log_level.upper()),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler('/tmp/ml_api_server.log')
        ]
    )
    
    logger.info("ğŸš€ ECG Audio Analyzer ML API ì„œë²„ ì‹œì‘")
    logger.info(f"   í˜¸ìŠ¤íŠ¸: {args.host}:{args.port}")
    logger.info(f"   ì›Œì»¤ ìˆ˜: {args.workers}")
    logger.info(f"   ë¡œê·¸ ë ˆë²¨: {args.log_level}")
    
    # GPU í™•ì¸
    try:
        import torch
        if torch.cuda.is_available():
            logger.info(f"   GPU: {torch.cuda.device_count()}ê°œ ì‚¬ìš© ê°€ëŠ¥")
            for i in range(torch.cuda.device_count()):
                logger.info(f"      GPU {i}: {torch.cuda.get_device_name(i)}")
        else:
            logger.warning("   GPU: ì‚¬ìš© ë¶ˆê°€ (CPU ëª¨ë“œ)")
    except ImportError:
        logger.warning("   PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
    
    # FastAPI ì„œë²„ ì‹¤í–‰
    uvicorn.run(
        app,
        host=args.host,
        port=args.port,
        workers=args.workers,
        access_log=True,
        log_level=args.log_level
    )