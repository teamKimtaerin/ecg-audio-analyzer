#!/usr/bin/env python3
"""
EC2 ML ì„œë²„ - FastAPI ê¸°ë°˜

WhisperX, í™”ì ë¶„ë¦¬, ê°ì • ë¶„ì„ ë“±ì„ HTTP APIë¡œ ì œê³µí•˜ëŠ” ì„œë²„
ECS FastAPI ë°±ì—”ë“œë¡œë¶€í„° ìš”ì²­ì„ ë°›ì•„ JSONìœ¼ë¡œ ê²°ê³¼ ë°˜í™˜
"""

import asyncio
import os
import sys
import logging
import tempfile
import uuid
import requests
from pathlib import Path
from typing import Dict, Any, Optional, List
from datetime import datetime

import boto3
import uvicorn
from fastapi import FastAPI, HTTPException, BackgroundTasks, Query
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

try:
    from src.services.callback_pipeline import process_video_with_fastapi
    from src.api import AnalysisConfig
    from src.utils.logger import get_logger
except ImportError as e:
    print(f"Warning: Import failed: {e}")
    # Mock í´ë˜ìŠ¤ë“¤
    class AnalysisConfig:
        def __init__(self, **kwargs):
            for k, v in kwargs.items():
                setattr(self, k, v)
    
    def get_logger(name):
        return logging.getLogger(name)

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="ECG Model Server",
    description="EC2 ML ì„œë²„ - ë¹„ë””ì˜¤ ì˜¤ë””ì˜¤ ë¶„ì„ API",
    version="1.0.0"
)

# CORS ì„¤ì • - ECS ë°±ì—”ë“œì™€ì˜ í†µì‹ ì„ ìœ„í•´
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ìš´ì˜ í™˜ê²½ì—ì„œëŠ” íŠ¹ì • ë„ë©”ì¸ìœ¼ë¡œ ì œí•œ
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# AWS S3 ë° Backend ì„¤ì •
s3_client = boto3.client('s3')
S3_BUCKET = "ecg-project-pipeline-dev-video-storage-np9digv7"
BACKEND_URL = "http://ecg-project-pipeline-dev-alb-1703405864.us-east-1.elb.amazonaws.com"

# In-memory job tracking
jobs: Dict[str, Dict[str, Any]] = {}

# ë¡œê±° ì„¤ì •
logger = get_logger(__name__)

# =============================================================================
# Pydantic ëª¨ë¸ë“¤ (íƒ€ì… ì–´ë…¸í…Œì´ì…˜ í¬í•¨)
# =============================================================================

class ProcessRequest(BaseModel):
    """ë¹„ë””ì˜¤ ë¶„ì„ ìš”ì²­ - ML_API.md ëª…ì„¸ ì¤€ìˆ˜"""
    job_id: str = Field(..., description="ì‘ì—… ID")
    video_url: str = Field(..., description="S3 ë¹„ë””ì˜¤ URL")
    enable_gpu: Optional[bool] = Field(True, description="GPU ì‚¬ìš© ì—¬ë¶€")
    emotion_detection: Optional[bool] = Field(True, description="ê°ì • ë¶„ì„ ì—¬ë¶€")
    language: Optional[str] = Field("auto", description="ì–¸ì–´ ì„¤ì •")
    max_workers: Optional[int] = Field(4, description="ìµœëŒ€ ì›Œì»¤ ìˆ˜")

class ProcessResponse(BaseModel):
    """ë¹„ë””ì˜¤ ë¶„ì„ ì‘ë‹µ - ML_API.md ëª…ì„¸ ì¤€ìˆ˜"""
    job_id: str = Field(..., description="ì‘ì—… ID")
    status: str = Field("processing", description="ì‘ì—… ìƒíƒœ")

class JobStatusResponse(BaseModel):
    """ì‘ì—… ìƒíƒœ ì‘ë‹µ - ML_API.md ëª…ì„¸ ì¤€ìˆ˜"""
    job_id: str = Field(..., description="ì‘ì—… ID")
    status: str = Field(..., description="ì‘ì—… ìƒíƒœ: processing, completed, failed")
    progress: Optional[int] = Field(None, description="ì§„í–‰ë¥  (0-100)")
    result: Optional[Dict[str, Any]] = Field(None, description="ë¶„ì„ ê²°ê³¼ (ì™„ë£Œì‹œ)")
    error: Optional[str] = Field(None, description="ì—ëŸ¬ ë©”ì‹œì§€ (ì‹¤íŒ¨ì‹œ)")

class MLResultRequest(BaseModel):
    """ML ì„œë²„ ê²°ê³¼ ìš”ì²­ (ë°±ì—”ë“œ ì½œë°±ìš©) - ML_API.md ëª…ì„¸ ì¤€ìˆ˜"""
    job_id: str = Field(..., description="ì‘ì—… ID")
    result: Dict[str, Any] = Field(..., description="Whisper ì›ë³¸ JSON ê²°ê³¼")

class HealthResponse(BaseModel):
    """í—¬ìŠ¤ ì²´í¬ ì‘ë‹µ"""
    status: str = Field(..., description="ì„œë²„ ìƒíƒœ")
    service: str = Field(..., description="ì„œë¹„ìŠ¤ ì´ë¦„")
    version: str = Field(..., description="ë²„ì „")
    gpu_available: Optional[bool] = Field(None, description="GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€")
    gpu_count: Optional[int] = Field(None, description="GPU ê°œìˆ˜")
    timestamp: str = Field(..., description="ì‘ë‹µ ì‹œê°„")

# =============================================================================
# API ì—”ë“œí¬ì¸íŠ¸ë“¤
# =============================================================================

@app.post("/api/upload-video/process-video", response_model=ProcessResponse)
async def process_video(
    request: ProcessRequest,
    background_tasks: BackgroundTasks = None
):
    """
    ë¹„ë™ê¸° ë¹„ë””ì˜¤ ë¶„ì„ ìš”ì²­ - ML_API.md ëª…ì„¸ ì¤€ìˆ˜
    
    Backendê°€ í˜¸ì¶œí•˜ëŠ” API:
    POST /api/upload-video/process-video
    {
        "job_id": "123456",
        "video_url": "https://bucket.s3.amazonaws.com/file.mp4"
    }
    """
    try:
        # ì‘ì—… ìƒíƒœ ì´ˆê¸°í™”
        jobs[request.job_id] = {
            "status": "processing",
            "video_url": request.video_url,
            "progress": 0,
            "created_at": datetime.now().isoformat()
        }
        
        logger.info(f"ë¹„ë™ê¸° ë¶„ì„ ìš”ì²­ ì‹œì‘ - job_id: {request.job_id}, video_url: {request.video_url}")
        
        # ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì‹œì‘
        if background_tasks:
            background_tasks.add_task(process_video_async, request.job_id, request.video_url)
        else:
            # ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì§ì ‘ ì‹¤í–‰
            asyncio.create_task(process_video_async(request.job_id, request.video_url))
        
        return ProcessResponse(
            job_id=request.job_id,
            status="processing"
        )
        
    except Exception as e:
        logger.error(f"ë¶„ì„ ìš”ì²­ ì‹¤íŒ¨ - Error: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"ë¶„ì„ ìš”ì²­ ì‹¤íŒ¨: {str(e)}"
        )

@app.get("/api/upload-video/status/{job_id}", response_model=JobStatusResponse)
async def get_job_status(job_id: str):
    """ì‘ì—… ìƒíƒœ ì¡°íšŒ - ML_API.md ëª…ì„¸ ì¤€ìˆ˜"""
    if job_id not in jobs:
        raise HTTPException(status_code=404, detail="ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    job = jobs[job_id]
    return JobStatusResponse(
        job_id=job_id,
        status=job["status"],
        progress=job.get("progress"),
        result=job.get("result"),
        error=job.get("error")
    )

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """ML ì„œë²„ í—¬ìŠ¤ ì²´í¬ - Backend í˜¸í™˜"""
    
    # GPU ì •ë³´ í™•ì¸
    gpu_available = False
    gpu_count = 0
    
    try:
        import torch
        if torch.cuda.is_available():
            gpu_available = True
            gpu_count = torch.cuda.device_count()
    except ImportError:
        pass
    
    return HealthResponse(
        status="healthy",
        service="ecg-ml-server",
        version="1.0.0",
        gpu_available=gpu_available,
        gpu_count=gpu_count,
        timestamp=datetime.now().isoformat()
    )

@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "service": "ECG Audio Analyzer ML API",
        "version": "1.0.0", 
        "description": "EC2 ML ì„œë²„ - ë¹„ë””ì˜¤ ì˜¤ë””ì˜¤ ë¶„ì„ API",
        "endpoints": {
            "process-video": "POST /api/upload-video/process-video - ë¹„ë™ê¸° ë¹„ë””ì˜¤ ë¶„ì„",
            "job-status": "GET /api/upload-video/status/{job_id} - ì‘ì—… ìƒíƒœ ì¡°íšŒ",
            "health": "GET /health - í—¬ìŠ¤ ì²´í¬",
            "docs": "GET /docs - API ë¬¸ì„œ"
        },
        "backend_integration": {
            "s3_bucket": S3_BUCKET,
            "backend_url": BACKEND_URL,
            "active_jobs": len([j for j in jobs.values() if j["status"] == "processing"])
        }
    }

@app.post("/test")
async def test_endpoint():
    """í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "status": "ok",
        "message": "ML API ì„œë²„ê°€ ì •ìƒ ì‘ë™ì¤‘ì…ë‹ˆë‹¤",
        "timestamp": datetime.now().isoformat(),
        "server_info": {
            "python_version": sys.version,
            "working_directory": str(Path.cwd()),
            "environment": os.environ.get("ENVIRONMENT", "development")
        }
    }

# =============================================================================
# ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ í•¨ìˆ˜ë“¤
# =============================================================================

async def process_video_async(job_id: str, video_url: str):
    """
    ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë¹„ë””ì˜¤ ë¶„ì„ ë° Backendë¡œ ê²°ê³¼ ì „ì†¡
    """
    try:
        logger.info(f"ë°±ê·¸ë¼ìš´ë“œ ë¶„ì„ ì‹œì‘ - job_id: {job_id}, video_url: {video_url}")
        
        # ì§„í–‰ìƒí™© ì—…ë°ì´íŠ¸
        jobs[job_id]["progress"] = 10
        
        # S3 URLì—ì„œ íŒŒì¼ í‚¤ ì¶”ì¶œ
        if video_url.startswith("https://"):
            # URLì—ì„œ íŒŒì¼ í‚¤ ì¶”ì¶œ: https://bucket.s3.amazonaws.com/path/file.mp4 -> path/file.mp4
            file_key = "/".join(video_url.split("/")[4:])
        else:
            file_key = video_url  # ì´ë¯¸ íŒŒì¼ í‚¤ì¸ ê²½ìš°
            
        # ì§„í–‰ìƒí™© ì—…ë°ì´íŠ¸
        jobs[job_id]["progress"] = 20
        
        # ì„ì‹œ íŒŒì¼ì— S3ì—ì„œ ë¹„ë””ì˜¤ ë‹¤ìš´ë¡œë“œ
        with tempfile.NamedTemporaryFile(suffix='.mp4', delete=False) as temp_file:
            temp_path = temp_file.name
            
            try:
                logger.info(f"S3ì—ì„œ ë¹„ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ì‹œì‘ - Bucket: {S3_BUCKET}, Key: {file_key}")
                s3_client.download_file(S3_BUCKET, file_key, temp_path)
                
                # ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ ê²€ì¦
                file_size = os.path.getsize(temp_path)
                logger.info(f"S3ì—ì„œ ë¹„ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ - íŒŒì¼ í¬ê¸°: {file_size} bytes, ê²½ë¡œ: {temp_path}")
                
                if file_size == 0:
                    raise Exception("ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
                    
                jobs[job_id]["progress"] = 40
                
            except Exception as s3_error:
                logger.error(f"S3 ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ - Bucket: {S3_BUCKET}, Key: {file_key}, Error: {s3_error}")
                jobs[job_id].update({
                    "status": "failed",
                    "progress": 100,
                    "error": f"S3 íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(s3_error)}",
                    "failed_at": datetime.now().isoformat()
                })
                return
            
            try:
                # ì§„í–‰ìƒí™© ì—…ë°ì´íŠ¸
                jobs[job_id]["progress"] = 60
                
                # ì‹¤ì œ ë¶„ì„ ì‹¤í–‰ ì‹œë„
                result = await run_actual_analysis(temp_path, file_key, job_id)
                
                # ğŸ” ê²°ê³¼ ê²€ì¦ ë° ë¡œê¹… ê°•í™”
                logger.info(f"ë¶„ì„ ê²°ê³¼ íƒ€ì…: {type(result)}")
                logger.info(f"ë¶„ì„ ê²°ê³¼ ë‚´ìš© (ì²˜ìŒ 500ì): {str(result)[:500]}")
                
                if result is None:
                    logger.error("ë¶„ì„ ê²°ê³¼ê°€ Noneì…ë‹ˆë‹¤!")
                    raise Exception("ë¶„ì„ ê²°ê³¼ê°€ Noneì…ë‹ˆë‹¤")
                
                # ì§„í–‰ìƒí™© ì—…ë°ì´íŠ¸
                jobs[job_id]["progress"] = 90
                
                # ê²°ê³¼ë¥¼ Whisper í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                if hasattr(result, 'model_dump'):
                    # Pydantic ëª¨ë¸ì¸ ê²½ìš°
                    whisper_result = result.model_dump()
                    logger.info("Pydantic ëª¨ë¸ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜")
                elif hasattr(result, '__dict__'):
                    # ì¼ë°˜ ê°ì²´ì¸ ê²½ìš°
                    whisper_result = result.__dict__
                    logger.info("ê°ì²´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜")
                else:
                    # ì´ë¯¸ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš°
                    whisper_result = result
                    logger.info("ì´ë¯¸ ë”•ì…”ë„ˆë¦¬ í˜•íƒœ")
                
                logger.info(f"ë³€í™˜ëœ ê²°ê³¼ í‚¤: {list(whisper_result.keys()) if isinstance(whisper_result, dict) else 'Not a dict'}")
                
                # ì„±ê³µì ì¸ ë¶„ì„ ê²°ê³¼ - ML_API.md ëª…ì„¸ì— ë§ê²Œ ë‹¨ìˆœí™”
                backend_result = {
                    "job_id": job_id,
                    "result": whisper_result  # ë³€í™˜ëœ ê²°ê³¼
                }
                
            except Exception as analysis_error:
                logger.error(f"WhisperX ë¶„ì„ ì‹¤íŒ¨ - job_id: {job_id}, Error: {analysis_error}")
                logger.error(f"ë¶„ì„ ì‹¤íŒ¨ ìƒì„¸ ì •ë³´ - Type: {type(analysis_error).__name__}, Args: {analysis_error.args}")
                
                jobs[job_id].update({
                    "status": "failed",
                    "progress": 100,
                    "error": f"ë¶„ì„ ì‹¤íŒ¨: {str(analysis_error)}",
                    "failed_at": datetime.now().isoformat()
                })
                return
            
            finally:
                # ì„ì‹œ íŒŒì¼ ì •ë¦¬
                try:
                    os.unlink(temp_path)
                except:
                    pass
            
            # ì‘ì—… ìƒíƒœ ì—…ë°ì´íŠ¸
            jobs[job_id].update({
                "status": "completed",
                "progress": 100,
                "result": backend_result,
                "video_url": video_url,
                "completed_at": datetime.now().isoformat()
            })
            
            # Backendë¡œ ê²°ê³¼ ì „ì†¡
            await send_result_to_backend(backend_result)
            
            logger.info(f"ë¶„ì„ ì™„ë£Œ - job_id: {job_id}")
            
    except Exception as e:
        jobs[job_id].update({
            "status": "failed",
            "progress": 100,
            "error": str(e),
            "video_url": video_url,
            "failed_at": datetime.now().isoformat()
        })
        logger.error(f"ë¶„ì„ ì‹¤íŒ¨ - job_id: {job_id}, error: {str(e)}")

async def run_actual_analysis(temp_path: str, file_key: str, job_id: str):
    """ì‹¤ì œ WhisperX ë¶„ì„ ì‹¤í–‰"""
    logger.info(f"WhisperX ë¶„ì„ ì‹œì‘ - job_id: {job_id}, íŒŒì¼: {temp_path}")
    
    try:
        # ì…ë ¥ íŒŒì¼ ê²€ì¦
        if not os.path.exists(temp_path):
            raise FileNotFoundError(f"ë¶„ì„í•  íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {temp_path}")
            
        file_size = os.path.getsize(temp_path)
        logger.info(f"ë¶„ì„ ëŒ€ìƒ íŒŒì¼ ì •ë³´ - í¬ê¸°: {file_size} bytes, ê²½ë¡œ: {temp_path}")
        
        # ë¶„ì„ ì„¤ì • ìƒì„±
        config = AnalysisConfig(
            enable_gpu=True,
            emotion_detection=True,
            language="auto",
            max_workers=4
        )
        logger.info(f"ë¶„ì„ ì„¤ì • - GPU: {config.enable_gpu}, ê°ì •ë¶„ì„: {config.emotion_detection}, ì–¸ì–´: {config.language}")
        
        # WhisperX íŒŒì´í”„ë¼ì¸ ëª¨ë“ˆ ì„í¬íŠ¸
        try:
            from src.services.callback_pipeline import CallbackPipelineManager
            logger.info("CallbackPipelineManager ëª¨ë“ˆ ë¡œë”© ì„±ê³µ")
        except ImportError as import_error:
            logger.error(f"CallbackPipelineManager ëª¨ë“ˆ ë¡œë”© ì‹¤íŒ¨: {import_error}")
            raise Exception(f"í•„ìˆ˜ ëª¨ë“ˆ ë¡œë”© ì‹¤íŒ¨: {import_error}")
        
        # íŒŒì´í”„ë¼ì¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        pipeline = CallbackPipelineManager(config, fastapi_client=None)
        logger.info("WhisperX íŒŒì´í”„ë¼ì¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì™„ë£Œ")
        
        # ì§„í–‰ìƒí™© ì½œë°± í•¨ìˆ˜ (ìƒì„¸ ë¡œê¹…)
        async def progress_callback(stage: str, progress: float, message: str = None):
            # 60-85% ë²”ìœ„ì—ì„œ ì§„í–‰ìƒí™© ì—…ë°ì´íŠ¸
            actual_progress = 60 + int(progress * 25)
            jobs[job_id]["progress"] = min(85, actual_progress)
            
            detailed_message = f"[{stage}] {progress:.1%} - {message or 'Processing...'}"
            logger.info(f"ë¶„ì„ ì§„í–‰ìƒí™© - job_id: {job_id}, {detailed_message}")
        
        # WhisperX íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        logger.info(f"WhisperX íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘ - job_id: {job_id}")
        start_time = datetime.now()
        
        result = await pipeline._run_pipeline_with_progress(
            input_source=temp_path,
            progress_callback=progress_callback,
            output_path=None
        )
        
        end_time = datetime.now()
        processing_duration = (end_time - start_time).total_seconds()
        logger.info(f"WhisperX íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ - job_id: {job_id}, ì²˜ë¦¬ì‹œê°„: {processing_duration:.2f}ì´ˆ")
        
        return result
        
    except ImportError as e:
        logger.error(f"ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨ - job_id: {job_id}, Error: {e}")
        raise Exception(f"í•„ìˆ˜ ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
    except FileNotFoundError as e:
        logger.error(f"íŒŒì¼ ì˜¤ë¥˜ - job_id: {job_id}, Error: {e}")
        raise Exception(f"íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
    except Exception as e:
        logger.error(f"ë¶„ì„ ì‹¤í–‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ - job_id: {job_id}, Type: {type(e).__name__}, Error: {e}")
        logger.error(f"ì˜ˆì™¸ ìƒì„¸ ì •ë³´ - Args: {e.args}")
        raise



async def send_result_to_backend(result: Dict[str, Any]):
    """Backendë¡œ ê²°ê³¼ ì „ì†¡ - ML_API.md ëª…ì„¸ ì¤€ìˆ˜"""
    job_id = result.get('job_id', 'unknown')
    
    try:
        logger.info(f"Backend ê²°ê³¼ ì „ì†¡ ì‹œì‘ - job_id: {job_id}, URL: {BACKEND_URL}/api/upload-video/result")
        
        # ê²°ê³¼ ë°ì´í„° í¬ê¸° ë¡œê¹…
        result_size = len(str(result))
        logger.info(f"ì „ì†¡í•  ë°ì´í„° í¬ê¸° - job_id: {job_id}, Size: {result_size} bytes")
        
        response = requests.post(
            f"{BACKEND_URL}/api/upload-video/result",
            json=result,
            timeout=30,
            headers={"Content-Type": "application/json"}
        )
        
        logger.info(f"Backend ì‘ë‹µ ìˆ˜ì‹  - job_id: {job_id}, Status: {response.status_code}")
        
        if response.status_code == 200:
            logger.info(f"Backendë¡œ ê²°ê³¼ ì „ì†¡ ì„±ê³µ - job_id: {job_id}")
            try:
                response_data = response.json()
                logger.info(f"Backend ì‘ë‹µ ë°ì´í„° - job_id: {job_id}, Response: {response_data}")
            except:
                logger.info(f"Backend ì‘ë‹µ í…ìŠ¤íŠ¸ - job_id: {job_id}, Response: {response.text}")
        else:
            logger.error(f"Backend ê²°ê³¼ ì „ì†¡ ì‹¤íŒ¨ - job_id: {job_id}, Status: {response.status_code}")
            logger.error(f"Backend ì˜¤ë¥˜ ì‘ë‹µ - job_id: {job_id}, Response: {response.text}")
            
    except requests.exceptions.Timeout as timeout_error:
        logger.error(f"Backend ê²°ê³¼ ì „ì†¡ íƒ€ì„ì•„ì›ƒ - job_id: {job_id}, Error: {timeout_error}")
    except requests.exceptions.ConnectionError as conn_error:
        logger.error(f"Backend ì—°ê²° ì‹¤íŒ¨ - job_id: {job_id}, URL: {BACKEND_URL}, Error: {conn_error}")
    except requests.exceptions.RequestException as req_error:
        logger.error(f"Backend ìš”ì²­ ì˜¤ë¥˜ - job_id: {job_id}, Error: {req_error}")
    except Exception as backend_error:
        logger.error(f"Backend ê²°ê³¼ ì „ì†¡ ì˜ˆì™¸ - job_id: {job_id}, Type: {type(backend_error).__name__}, Error: {backend_error}")


# =============================================================================
# ë ˆê±°ì‹œ ì—”ë“œí¬ì¸íŠ¸ (í˜¸í™˜ì„± ìœ ì§€)
# =============================================================================

@app.post("/request-process")
async def request_process_legacy(
    fileKey: str = Query(..., description="S3 íŒŒì¼ í‚¤"),
    background_tasks: BackgroundTasks = None
):
    """ë ˆê±°ì‹œ ì—”ë“œí¬ì¸íŠ¸ (í˜¸í™˜ì„± ìœ ì§€)"""
    # ê¸°ì¡´ í˜•ì‹ì„ ìƒˆ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    job_id = str(uuid.uuid4())
    # S3 URL í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ê°„ë‹¨í•œ ì¶”ì •)
    video_url = f"https://{S3_BUCKET}.s3.amazonaws.com/{fileKey}"
    
    request = ProcessRequest(
        job_id=job_id,
        video_url=video_url
    )
    
    return await process_video(request, background_tasks)

# =============================================================================
# ì„œë²„ ì‹¤í–‰ ì½”ë“œ
# =============================================================================

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="ECG Audio Analyzer ML API Server")
    parser.add_argument("--host", default="0.0.0.0", help="ë°”ì¸ë“œ í˜¸ìŠ¤íŠ¸")
    parser.add_argument("--port", type=int, default=8080, help="ë°”ì¸ë“œ í¬íŠ¸")
    parser.add_argument("--workers", type=int, default=1, help="ì›Œì»¤ ìˆ˜")
    parser.add_argument("--log-level", default="info", choices=["debug", "info", "warning", "error"])
    
    args = parser.parse_args()
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=getattr(logging, args.log_level.upper()),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler('/tmp/ml_api_server.log')
        ]
    )
    
    logger.info("ğŸš€ ECG Audio Analyzer ML API ì„œë²„ ì‹œì‘")
    logger.info(f"   í˜¸ìŠ¤íŠ¸: {args.host}:{args.port}")
    logger.info(f"   ì›Œì»¤ ìˆ˜: {args.workers}")
    logger.info(f"   ë¡œê·¸ ë ˆë²¨: {args.log_level}")
    
    # GPU í™•ì¸
    try:
        import torch
        if torch.cuda.is_available():
            logger.info(f"   GPU: {torch.cuda.device_count()}ê°œ ì‚¬ìš© ê°€ëŠ¥")
            for i in range(torch.cuda.device_count()):
                logger.info(f"      GPU {i}: {torch.cuda.get_device_name(i)}")
        else:
            logger.warning("   GPU: ì‚¬ìš© ë¶ˆê°€ (CPU ëª¨ë“œ)")
    except ImportError:
        logger.warning("   PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
    
    # FastAPI ì„œë²„ ì‹¤í–‰
    uvicorn.run(
        app,
        host=args.host,
        port=args.port,
        workers=args.workers,
        access_log=True,
        log_level=args.log_level
    )