"""
EC2 ML ì„œë²„ - FastAPI ê¸°ë°˜

WhisperX, í™”ì ë¶„ë¦¬, ê°ì • ë¶„ì„ ë“±ì„ HTTP APIë¡œ ì œê³µí•˜ëŠ” ì„œë²„
ECS FastAPI ë°±ì—”ë“œë¡œë¶€í„° ìš”ì²­ì„ ë°›ì•„ JSONìœ¼ë¡œ ê²°ê³¼ ë°˜í™˜
"""

import asyncio
import os
import sys
import logging
from pathlib import Path
from typing import Dict, Any, Optional, Union
from datetime import datetime

from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel
import uvicorn

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.services.callback_pipeline import process_video_with_fastapi
from src.api import AnalysisConfig
from src.utils.logger import get_logger

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="ECG Audio Analyzer ML API",
    description="EC2 ML ì„œë²„ - ë¹„ë””ì˜¤ ì˜¤ë””ì˜¤ ë¶„ì„ API",
    version="1.0.0"
)

# ë¡œê±° ì„¤ì •
logger = get_logger(__name__)


# Pydantic ëª¨ë¸ë“¤
class TranscribeRequest(BaseModel):
    """ë¹„ë””ì˜¤ ë¶„ì„ ìš”ì²­"""
    video_path: Optional[str] = None
    video_url: Optional[str] = None
    
    # ë¶„ì„ ì„¤ì •
    enable_gpu: bool = True
    emotion_detection: bool = True
    language: str = "auto"
    max_workers: int = 4
    
    # ì¶œë ¥ ì„¤ì •
    output_path: Optional[str] = None


class TranscribeResponse(BaseModel):
    """ë¹„ë””ì˜¤ ë¶„ì„ ì‘ë‹µ"""
    success: bool
    processing_time: float
    metadata: Dict[str, Any]
    transcript_segments: list
    speaker_segments: list
    emotion_segments: list
    acoustic_features: list
    performance_stats: Optional[Dict[str, Any]] = None


class HealthResponse(BaseModel):
    """í—¬ìŠ¤ ì²´í¬ ì‘ë‹µ"""
    status: str
    service: str
    version: str
    gpu_available: bool
    gpu_count: int
    timestamp: str


@app.post("/transcribe", response_model=TranscribeResponse)
async def transcribe_video(request: TranscribeRequest):
    """
    ë¹„ë””ì˜¤ ì˜¤ë””ì˜¤ ë¶„ì„ ë©”ì¸ ì—”ë“œí¬ì¸íŠ¸
    
    ECS FastAPI ë°±ì—”ë“œê°€ í˜¸ì¶œí•˜ëŠ” API:
    POST http://ec2-ml-server:8001/transcribe
    {
        "video_path": "/path/to/video.mp4",
        "enable_gpu": true,
        "emotion_detection": true,
        "language": "auto"
    }
    """
    
    start_time = datetime.now()
    
    try:
        # ì…ë ¥ ê²€ì¦
        if not request.video_path and not request.video_url:
            raise HTTPException(
                status_code=400,
                detail="video_path ë˜ëŠ” video_url ì¤‘ í•˜ë‚˜ëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤"
            )
        
        input_source = request.video_path or request.video_url
        
        logger.info(f"ë¹„ë””ì˜¤ ë¶„ì„ ìš”ì²­ ìˆ˜ì‹  - Source: {input_source}")
        
        # ë¶„ì„ ì„¤ì • ìƒì„±
        config = AnalysisConfig(
            enable_gpu=request.enable_gpu,
            emotion_detection=request.emotion_detection,
            language=request.language,
            max_workers=request.max_workers
        )
        
        # ë¶„ì„ ì‹¤í–‰ (ì½œë°± ì—†ì´ ì§ì ‘ ì‹¤í–‰)
        from src.services.callback_pipeline import CallbackPipelineManager
        
        pipeline = CallbackPipelineManager(config, fastapi_client=None)  # ì½œë°± ì—†ìŒ
        
        # ì§ì ‘ ë¶„ì„ ì‹¤í–‰ (ì½œë°± ì—†ëŠ” ë²„ì „)
        result = await pipeline._run_pipeline_with_progress(
            input_source=input_source,
            progress_callback=_dummy_progress_callback,
            output_path=request.output_path
        )
        
        # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
        processing_time = (datetime.now() - start_time).total_seconds()
        
        logger.info(f"ë¹„ë””ì˜¤ ë¶„ì„ ì™„ë£Œ - ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ")
        
        # ì‘ë‹µ ìƒì„±
        response = TranscribeResponse(
            success=True,
            processing_time=processing_time,
            metadata={
                "video_path": str(result.metadata.video_path) if result.metadata.video_path else None,
                "processing_time": result.metadata.processing_time,
                "analysis_timestamp": result.metadata.analysis_timestamp.isoformat() if result.metadata.analysis_timestamp else None,
                "total_speakers": result.metadata.total_speakers,
                "video_duration": result.metadata.video_duration,
                "audio_duration": result.metadata.audio_duration,
                "language_detected": result.metadata.language_detected
            },
            transcript_segments=[
                {
                    "start_time": seg.start_time,
                    "end_time": seg.end_time,
                    "text": seg.text,
                    "confidence": seg.confidence,
                    "speaker_id": seg.speaker_id
                }
                for seg in (result.transcript_segments or [])
            ],
            speaker_segments=[
                {
                    "start_time": seg.start_time,
                    "end_time": seg.end_time,
                    "speaker_id": seg.speaker_id,
                    "confidence": seg.confidence
                }
                for seg in (result.speaker_segments or [])
            ],
            emotion_segments=[
                {
                    "start_time": seg.start_time,
                    "end_time": seg.end_time,
                    "emotion": seg.emotion,
                    "confidence": seg.confidence,
                    "valence": seg.valence,
                    "arousal": seg.arousal
                }
                for seg in (result.emotion_segments or [])
            ],
            acoustic_features=[
                # ìŒí–¥ íŠ¹ì„± ë°ì´í„° (í•„ìš”ì‹œ ì¶”ê°€)
            ],
            performance_stats={
                "cpu_time": result.performance_stats.cpu_time if result.performance_stats else None,
                "memory_peak_mb": result.performance_stats.memory_peak_mb if result.performance_stats else None,
                "gpu_time": result.performance_stats.gpu_time if result.performance_stats else None,
                "total_processing_time": result.performance_stats.total_processing_time if result.performance_stats else None
            } if result.performance_stats else None
        )
        
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë¹„ë””ì˜¤ ë¶„ì„ ì‹¤íŒ¨ - Error: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"ë¹„ë””ì˜¤ ë¶„ì„ ì‹¤íŒ¨: {str(e)}"
        )


async def _dummy_progress_callback(stage: str, progress: float, message: str = None):
    """ì§„í–‰ ìƒí™© ì½œë°± (ë¡œê·¸ë§Œ ì¶œë ¥)"""
    logger.info(f"Progress: {stage} - {progress:.1%} - {message or ''}")


@app.get("/health", response_model=HealthResponse)
async def health_check():
    """ML ì„œë²„ í—¬ìŠ¤ ì²´í¬"""
    
    try:
        import torch
        gpu_available = torch.cuda.is_available()
        gpu_count = torch.cuda.device_count() if gpu_available else 0
        
        return HealthResponse(
            status="healthy",
            service="ECG Audio Analyzer ML API",
            version="1.0.0",
            gpu_available=gpu_available,
            gpu_count=gpu_count,
            timestamp=datetime.now().isoformat()
        )
    except Exception as e:
        logger.error(f"í—¬ìŠ¤ ì²´í¬ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail="í—¬ìŠ¤ ì²´í¬ ì‹¤íŒ¨")


@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "service": "ECG Audio Analyzer ML API",
        "version": "1.0.0", 
        "description": "EC2 ML ì„œë²„ - ë¹„ë””ì˜¤ ì˜¤ë””ì˜¤ ë¶„ì„ API",
        "endpoints": {
            "transcribe": "POST /transcribe - ë¹„ë””ì˜¤ ì˜¤ë””ì˜¤ ë¶„ì„",
            "health": "GET /health - í—¬ìŠ¤ ì²´í¬",
            "docs": "GET /docs - API ë¬¸ì„œ"
        }
    }


@app.post("/test")
async def test_endpoint():
    """í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "status": "ok",
        "message": "ML API ì„œë²„ê°€ ì •ìƒ ì‘ë™ì¤‘ì…ë‹ˆë‹¤",
        "timestamp": datetime.now().isoformat()
    }


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="ECG Audio Analyzer ML API Server")
    parser.add_argument("--host", default="0.0.0.0", help="ë°”ì¸ë“œ í˜¸ìŠ¤íŠ¸")
    parser.add_argument("--port", type=int, default=8001, help="ë°”ì¸ë“œ í¬íŠ¸") 
    parser.add_argument("--workers", type=int, default=1, help="ì›Œì»¤ ìˆ˜")
    parser.add_argument("--log-level", default="info", choices=["debug", "info", "warning", "error"])
    
    args = parser.parse_args()
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=getattr(logging, args.log_level.upper()),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    logger.info(f"ğŸš€ ECG Audio Analyzer ML API ì„œë²„ ì‹œì‘")
    logger.info(f"   í˜¸ìŠ¤íŠ¸: {args.host}:{args.port}")
    logger.info(f"   ì›Œì»¤ ìˆ˜: {args.workers}")
    logger.info(f"   ë¡œê·¸ ë ˆë²¨: {args.log_level}")
    
    # GPU í™•ì¸
    try:
        import torch
        if torch.cuda.is_available():
            logger.info(f"   GPU: {torch.cuda.device_count()}ê°œ ì‚¬ìš© ê°€ëŠ¥")
        else:
            logger.warning("   GPU: ì‚¬ìš© ë¶ˆê°€ (CPU ëª¨ë“œ)")
    except ImportError:
        logger.warning("   PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
    
    # FastAPI ì„œë²„ ì‹¤í–‰
    uvicorn.run(
        app,
        host=args.host,
        port=args.port,
        workers=args.workers,
        access_log=True,
        log_level=args.log_level
    )