"""
EC2 ML ì„œë²„ - FastAPI ê¸°ë°˜

WhisperX, í™”ì ë¶„ë¦¬, ê°ì • ë¶„ì„ ë“±ì„ HTTP APIë¡œ ì œê³µí•˜ëŠ” ì„œë²„
ECS FastAPI ë°±ì—”ë“œë¡œë¶€í„° ìš”ì²­ì„ ë°›ì•„ JSONìœ¼ë¡œ ê²°ê³¼ ë°˜í™˜
"""

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€ (ë‹¤ë¥¸ import ì „ì— ì‹¤í–‰)
import sys
from pathlib import Path
import json

project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

import logging
import os
from typing import Dict, Any, Optional, cast
from datetime import datetime
import random

from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel
import uvicorn
import uuid
import requests
import tempfile
import boto3

from src.api import AnalysisConfig
from src.utils.logger import get_logger
from src.models.output_models import AudioFeatures

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="ECG Model Server",
    description="EC2 ML ì„œë²„ - ë¹„ë””ì˜¤ ì˜¤ë””ì˜¤ ë¶„ì„ API",
    version="1.0.0",
)

# AWS S3 ë° Backend ì„¤ì •
s3_client = boto3.client("s3")
S3_BUCKET = "ecg-project-pipeline-dev-video-storage-np9div7"

# í™˜ê²½ë³€ìˆ˜ì—ì„œ Backend URL ê°€ì ¸ì˜¤ê¸° (ë‹¤ì–‘í•œ ì´ë¦„ ì§€ì›)
# ë¡œì»¬ ê°œë°œì„ ìœ„í•´ localhostë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •
BACKEND_URL = os.getenv(
    "BACKEND_URL",
    os.getenv(
        "ECG_BACKEND_URL",
        os.getenv(
            "ml_api_server_url",
            "http://localhost:8000",  # ë¡œì»¬ ê°œë°œìš© ê¸°ë³¸ê°’
        ),
    ),
)

# In-memory job tracking
jobs = {}

# ë¡œê±° ì„¤ì •
logger = get_logger(__name__)


# Pydantic ëª¨ë¸ë“¤
class ProcessVideoRequest(BaseModel):
    """Backend í˜¸í™˜ ë¹„ë””ì˜¤ ì²˜ë¦¬ ìš”ì²­ (API ëª…ì„¸ ì¤€ìˆ˜)"""
    job_id: str
    video_url: str


class ProcessVideoResponse(BaseModel):
    """Backend í˜¸í™˜ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì‘ë‹µ"""
    job_id: str
    status: str
    message: str
    estimated_time: Optional[int] = 300  # seconds


class MLProgressCallback(BaseModel):
    """ML ì§„í–‰ ìƒí™© ì½œë°±"""
    job_id: str
    status: str  # processing, completed, failed
    progress: int  # 0-100
    message: Optional[str] = None
    result: Optional[Dict[str, Any]] = None
    error_message: Optional[str] = None
    error_code: Optional[str] = None


class RequestProcessRequest(BaseModel):
    """ë¹„ë™ê¸° ë¹„ë””ì˜¤ ë¶„ì„ ìš”ì²­ (legacy)"""

    video_key: str  # S3 í‚¤

    # ë¶„ì„ ì„¤ì •
    enable_gpu: bool = True
    emotion_detection: bool = True
    language: str = "auto"
    max_workers: int = 4


class TranscribeRequest(BaseModel):
    """ë°±ì—”ë“œ í˜¸í™˜ ì „ì‚¬ ìš”ì²­"""

    video_path: str
    video_url: Optional[str] = None
    enable_gpu: bool = True
    emotion_detection: bool = True
    language: str = "auto"
    max_workers: int = 4
    output_path: Optional[str] = None


class RequestProcessResponse(BaseModel):
    """ë¹„ë™ê¸° ë¹„ë””ì˜¤ ë¶„ì„ ì‘ë‹µ"""

    job_id: str
    status: str  # "processing"


class TranscribeResponse(BaseModel):
    """ë¹„ë””ì˜¤ ë¶„ì„ ì‘ë‹µ"""

    success: bool
    processing_time: float
    metadata: Dict[str, Any]
    transcript_segments: list
    speaker_segments: list
    emotion_segments: list
    acoustic_features: list
    performance_stats: Optional[Dict[str, Any]] = None


class BackendTranscribeResponse(BaseModel):
    """ìƒì„¸ ë¶„ì„ ì‘ë‹µ (simplified)"""

    success: bool
    metadata: Optional[Dict[str, Any]] = None
    speakers: Optional[Dict[str, Any]] = None
    segments: Optional[list] = None
    processing_time: float
    error: Optional[str] = None
    error_code: Optional[str] = None


class HealthResponse(BaseModel):
    """í—¬ìŠ¤ ì²´í¬ ì‘ë‹µ"""

    status: str
    service: str
    version: str
    gpu_available: bool
    gpu_count: int
    timestamp: str


@app.post("/api/upload-video/process-video", response_model=ProcessVideoResponse)
async def process_video_api(request: ProcessVideoRequest, background_tasks: BackgroundTasks):
    """
    Backend í˜¸í™˜ ë¹„ë””ì˜¤ ì²˜ë¦¬ API (ëª…ì„¸ì„œ ì¤€ìˆ˜)
    
    POST {ML_SERVER_URL}/api/upload-video/process-video
    Content-Type: application/json
    
    Request Body:
    {
        "job_id": "550e8400-e29b-41d4-a716-446655440000",
        "video_url": "https://s3.amazonaws.com/..."
    }
    """
    try:
        job_id = request.job_id
        video_url = request.video_url
        
        # Job ìƒíƒœ ì¶”ì 
        jobs[job_id] = {
            "status": "accepted",
            "video_url": video_url,
            "started_at": datetime.now().isoformat()
        }
        
        logger.info(f"ë¹„ë””ì˜¤ ì²˜ë¦¬ ìš”ì²­ ì ‘ìˆ˜ - job_id: {job_id}, video_url: {video_url}")
        logger.info(f"Backend URL ì„¤ì •: {BACKEND_URL}")
        
        # ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì‹œì‘
        background_tasks.add_task(
            process_video_with_callback,
            job_id,
            video_url
        )
        
        return ProcessVideoResponse(
            job_id=job_id,
            status="accepted",
            message="Processing started",
            estimated_time=300
        )
        
    except Exception as e:
        logger.error(f"ì²˜ë¦¬ ìš”ì²­ ì‹¤íŒ¨ - job_id: {request.job_id}, error: {str(e)}")
        # ì‹¤íŒ¨ ì½œë°± ì „ì†¡
        await send_error_to_backend(request.job_id, str(e), "INVALID_REQUEST")
        raise HTTPException(
            status_code=400,
            detail={
                "error": {
                    "code": "INVALID_REQUEST",
                    "message": str(e),
                    "job_id": request.job_id
                }
            }
        )


@app.post("/request-process", response_model=RequestProcessResponse)
async def request_process(video_key: str, background_tasks: BackgroundTasks):
    """
    ë¹„ë™ê¸° ë¹„ë””ì˜¤ ë¶„ì„ ìš”ì²­ - Backendì™€ í˜¸í™˜ (legacy)

    Backendê°€ í˜¸ì¶œí•˜ëŠ” API:
    POST http://10.0.10.42:8080/request-process?video_key=uploads/uuid/video.mp4
    """

    try:
        # Job ID ìƒì„±
        job_id = str(uuid.uuid4())
        jobs[job_id] = {"status": "processing", "video_key": video_key}

        logger.info(f"ë¹„ë™ê¸° ë¶„ì„ ìš”ì²­ ì‹œì‘ - video_key: {video_key}, job_id: {job_id}")

        # ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì‹œì‘
        background_tasks.add_task(process_video_async, job_id, video_key)

        return RequestProcessResponse(job_id=job_id, status="processing")

    except Exception as e:
        logger.error(f"ë¶„ì„ ìš”ì²­ ì‹¤íŒ¨ - Error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ë¶„ì„ ìš”ì²­ ì‹¤íŒ¨: {str(e)}")


async def process_video_async(job_id: str, video_key: str):
    """
    ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë¹„ë””ì˜¤ ë¶„ì„ ë° Backendë¡œ ê²°ê³¼ ì „ì†¡
    """
    try:
        logger.info(f"ë°±ê·¸ë¼ìš´ë“œ ë¶„ì„ ì‹œì‘ - job_id: {job_id}, video_key: {video_key}")

        # ì„ì‹œ íŒŒì¼ì— S3ì—ì„œ ë¹„ë””ì˜¤ ë‹¤ìš´ë¡œë“œ
        with tempfile.NamedTemporaryFile(suffix=".mp4", delete=False) as temp_file:
            try:
                s3_client.download_file(S3_BUCKET, video_key, temp_file.name)
                logger.info(f"S3ì—ì„œ ë¹„ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {video_key}")
            except Exception as s3_error:
                logger.warning(f"S3 ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨, ëª¨í¬ ë°ì´í„° ì‚¬ìš©: {s3_error}")

            # ë¶„ì„ ì„¤ì • ìƒì„±
            config = AnalysisConfig(
                enable_gpu=True, emotion_detection=True, language="auto", max_workers=4
            )

            try:
                # ì‹¤ì œ ë¶„ì„ ì‹¤í–‰
                from src.services.callback_pipeline import CallbackPipelineManager

                pipeline = CallbackPipelineManager(config, fastapi_client=None)

                result = await pipeline._run_pipeline_with_progress(
                    input_source=temp_file.name,
                    progress_callback=_dummy_progress_callback,
                    output_path=None,
                )

                # ê²°ê³¼ë¥¼ Backend í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                # CompleteAnalysisResultëŠ” timeline ì†ì„±ì„ ê°€ì§
                timeline_segments = getattr(result, "timeline", [])

                backend_result = {
                    "job_id": job_id,
                    "video_key": video_key,
                    "status": "completed",
                    "success": True,
                    "processing_time": (
                        result.metadata.processing_time if result.metadata else 0
                    ),
                    "results": {
                        "transcript": [
                            {
                                "start": seg.start_time,
                                "end": seg.end_time,
                                "text": getattr(seg, "text_placeholder", ""),
                                "speaker": seg.speaker_id,
                            }
                            for seg in timeline_segments
                        ],
                        "emotions": [
                            {
                                "start": seg.start_time,
                                "end": seg.end_time,
                                "emotion": seg.emotion.emotion,
                                "confidence": seg.emotion.confidence,
                            }
                            for seg in timeline_segments
                        ],
                        "metadata": {
                            "duration": (
                                result.metadata.duration if result.metadata else 0
                            ),
                            "speakers": (
                                result.metadata.total_speakers if result.metadata else 0
                            ),
                            "language": "auto",
                        },
                    },
                }

            except Exception as analysis_error:
                logger.error(f"ë¶„ì„ ì‹¤íŒ¨: {analysis_error}")
                # ë¶„ì„ ì‹¤íŒ¨ì‹œ ëª¨í¬ ê²°ê³¼ ìƒì„±
                backend_result = {
                    "job_id": job_id,
                    "video_key": video_key,
                    "status": "completed",
                    "success": False,
                    "error": str(analysis_error),
                    "results": {
                        "transcript": [
                            {
                                "start": 0.0,
                                "end": 3.0,
                                "text": "ë¶„ì„ ì‹¤íŒ¨ - ëª¨í¬ ë°ì´í„°",
                                "speaker": "SPEAKER_00",
                            }
                        ],
                        "emotions": [],
                        "metadata": {
                            "duration": 0,
                            "speakers": 0,
                            "language": "unknown",
                        },
                    },
                }

            # ì‘ì—… ìƒíƒœ ì—…ë°ì´íŠ¸
            jobs[job_id] = {"status": "completed", "result": backend_result}

            # Backendë¡œ ê²°ê³¼ ì „ì†¡
            try:
                response = requests.post(
                    f"{BACKEND_URL}/api/upload-video/results",
                    json=backend_result,
                    timeout=30,
                )
                logger.info(
                    f"Backendë¡œ ê²°ê³¼ ì „ì†¡ ì™„ë£Œ - job_id: {job_id}, status: {response.status_code}"
                )
            except Exception as backend_error:
                logger.error(f"Backend ê²°ê³¼ ì „ì†¡ ì‹¤íŒ¨: {backend_error}")

            logger.info(f"ë¶„ì„ ì™„ë£Œ - job_id: {job_id}")

    except Exception as e:
        jobs[job_id] = {"status": "failed", "error": str(e)}
        logger.error(f"ë¶„ì„ ì‹¤íŒ¨ - job_id: {job_id}, error: {str(e)}")


async def _dummy_progress_callback(stage: str, progress: float, message: str = ""):
    """ì§„í–‰ ìƒí™© ì½œë°± (ë¡œê·¸ë§Œ ì¶œë ¥)"""
    logger.info(f"Progress: {stage} - {progress:.1%} - {message or ''}")


async def send_error_to_backend(job_id: str, error_message: str, error_code: str = "PROCESSING_ERROR"):
    """ë°±ì—”ë“œì— ì—ëŸ¬ ì „ì†¡"""
    try:
        payload = MLProgressCallback(
            job_id=job_id,
            status="failed",
            progress=0,
            error_message=error_message,
            error_code=error_code
        ).model_dump()
        
        response = requests.post(
            f"{BACKEND_URL}/api/v1/ml/ml-results",
            json=payload,
            headers={
                "Content-Type": "application/json",
                "User-Agent": "ML-Server/1.0"
            },
            timeout=10
        )
        
        if response.status_code == 200:
            logger.info(f"Error reported to backend for job {job_id}")
        else:
            logger.warning(f"Failed to report error: {response.status_code}")
            
    except Exception as e:
        logger.error(f"Failed to send error to backend: {str(e)}")


async def download_from_url(url: str, job_id: str) -> str:
    """URLì—ì„œ ë¹„ë””ì˜¤ ë‹¤ìš´ë¡œë“œ"""
    try:
        # Progress: ë‹¤ìš´ë¡œë“œ ì‹œì‘
        await send_progress_to_backend(job_id, 5, "ë¹„ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
        
        # ì„ì‹œ íŒŒì¼ ìƒì„±
        with tempfile.NamedTemporaryFile(suffix=".mp4", delete=False) as temp_file:
            response = requests.get(url, stream=True, timeout=60)
            response.raise_for_status()
            
            total_size = int(response.headers.get('content-length', 0))
            downloaded = 0
            
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    temp_file.write(chunk)
                    downloaded += len(chunk)
                    
                    # ë‹¤ìš´ë¡œë“œ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (5-10%)
                    if total_size > 0:
                        progress = 5 + int((downloaded / total_size) * 5)
                        await send_progress_to_backend(job_id, progress, f"ë‹¤ìš´ë¡œë“œ ì¤‘... ({downloaded}/{total_size} bytes)")
            
            logger.info(f"ë¹„ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {temp_file.name}")
            return temp_file.name
            
    except Exception as e:
        logger.error(f"ë¹„ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        raise Exception(f"Failed to download video: {str(e)}")


async def send_progress_to_backend(job_id: str, progress: int, message: str = ""):
    """ë°±ì—”ë“œì— ì§„í–‰ ìƒí™© ì „ì†¡ (API ëª…ì„¸ ì¤€ìˆ˜)"""
    try:
        payload = MLProgressCallback(
            job_id=job_id,
            status="processing",
            progress=progress,  # 0-100
            message=message or "ì²˜ë¦¬ ì¤‘..."
        ).model_dump()

        # ë¡œê¹…ì„ ìœ„í•´ ìš”ì²­ ì •ë³´ ì¶œë ¥
        callback_url = f"{BACKEND_URL}/api/v1/ml/ml-results"
        logger.debug(f"ì½œë°± ì „ì†¡ - URL: {callback_url}, í˜ì´ë¡œë“œ: {payload}")
        
        response = requests.post(
            callback_url,
            json=payload,
            headers={
                "Content-Type": "application/json",
                "User-Agent": "ML-Server/1.0"
            },
            timeout=10,
        )

        if response.status_code == 200:
            logger.info(f"âœ… Progress updated: {message} ({progress}%)")
        else:
            logger.warning(f"âš ï¸ Progress update failed: {response.status_code}")

    except Exception as e:
        logger.error(f"âŒ Progress update error: {str(e)}")


async def process_video_with_callback(job_id: str, video_url: str):
    """
    API ëª…ì„¸ì— ë”°ë¥¸ ë¹„ë””ì˜¤ ì²˜ë¦¬ ë° ì½œë°±
    """
    start_time = datetime.now()
    
    try:
        # 1. ë¹„ë””ì˜¤ ë‹¤ìš´ë¡œë“œ (0-10%)
        video_path = await download_from_url(video_url, job_id)
        
        # 2. ì˜¤ë””ì˜¤ ì¶”ì¶œ (10-25%)
        await send_progress_to_backend(job_id, 10, "ì˜¤ë””ì˜¤ ì¶”ì¶œ ì¤‘...")
        
        # 3. ìŒì„± êµ¬ê°„ ê°ì§€ (25-40%)
        await send_progress_to_backend(job_id, 25, "ìŒì„± êµ¬ê°„ ê°ì§€ ì¤‘...")
        
        # 4. í™”ì ì‹ë³„ (40-60%)
        await send_progress_to_backend(job_id, 40, "í™”ì ì‹ë³„ ì¤‘...")
        
        # ë¶„ì„ ì„¤ì •
        config = AnalysisConfig(
            enable_gpu=True,
            emotion_detection=True,
            language="auto",
            max_workers=4
        )
        
        # 5. ì „ì‚¬ ìƒì„± (60-75%)
        await send_progress_to_backend(job_id, 60, "ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ ì¤‘...")
        
        # Pipeline ì‹¤í–‰
        from src.services.callback_pipeline import CallbackPipelineManager
        
        async def progress_callback(stage: str, progress: float, message: str = ""):
            # 60-75% êµ¬ê°„ì— ë§¤í•‘
            adjusted = 60 + int(progress * 15)
            await send_progress_to_backend(job_id, adjusted, message or stage or "ì „ì‚¬ ì²˜ë¦¬ ì¤‘...")
        
        pipeline = CallbackPipelineManager(config, fastapi_client=None)
        result = await pipeline._run_pipeline_with_progress(
            input_source=video_path,
            progress_callback=progress_callback,
            output_path=None
        )
        
        # 6. ê°ì • ë° ì‹ ë¢°ë„ ë¶„ì„ (75-90%)
        await send_progress_to_backend(job_id, 75, "ê°ì • ë¶„ì„ ì¤‘...")
        
        # WhisperX ê²°ê³¼ ì¶”ì¶œ
        whisperx_result = None
        if hasattr(pipeline.pipeline_manager, '_last_whisperx_result'):
            whisperx_result = pipeline.pipeline_manager._last_whisperx_result
        
        # ê²°ê³¼ ë³€í™˜ (API ëª…ì„¸ ì¤€ìˆ˜)
        segments = []
        if whisperx_result and "segments" in whisperx_result:
            for seg in whisperx_result["segments"]:
                segment_data = {
                    "start_time": seg.get("start", 0.0),
                    "end_time": seg.get("end", 0.0),
                    "speaker": {
                        "speaker_id": seg.get("speaker", "SPEAKER_01")
                    },
                    "text": seg.get("text", "").strip(),
                    "words": []
                }
                
                # ë‹¨ì–´ë³„ ì •ë³´ ì¶”ê°€
                if "words" in seg:
                    for word in seg["words"]:
                        word_data = {
                            "word": word.get("word", ""),
                            "start": word.get("start", 0.0),
                            "end": word.get("end", 0.0),
                            "volume_db": -20.0 + random.uniform(-5, 5),  # ìƒ˜í”Œ ê°’
                            "pitch_hz": 150.0 + random.uniform(-50, 200)  # ìƒ˜í”Œ ê°’
                        }
                        segment_data["words"].append(word_data)
                
                segments.append(segment_data)
        
        # 7. ê²°ê³¼ ì •ë¦¬ (90-100%)
        await send_progress_to_backend(job_id, 90, "ê²°ê³¼ ì •ë¦¬ ì¤‘...")
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        # ìµœì¢… ê²°ê³¼ ì¤€ë¹„
        final_result = {
            "metadata": {
                "filename": Path(video_url).name if "/" in video_url else "video.mp4",
                "duration": result.metadata.duration if result.metadata else 0,
                "total_segments": len(segments),
                "unique_speakers": len(set(s["speaker"]["speaker_id"] for s in segments)) if segments else 0
            },
            "segments": segments
        }
        
        # ì™„ë£Œ ì½œë°± ì „ì†¡
        await send_progress_to_backend(job_id, 100, "ë¶„ì„ ì™„ë£Œ")
        
        # ìµœì¢… ê²°ê³¼ ì „ì†¡
        complete_payload = MLProgressCallback(
            job_id=job_id,
            status="completed",
            progress=100,
            result=final_result
        ).model_dump()
        
        response = requests.post(
            f"{BACKEND_URL}/api/v1/ml/ml-results",
            json=complete_payload,
            headers={
                "Content-Type": "application/json",
                "User-Agent": "ML-Server/1.0"
            },
            timeout=30
        )
        
        if response.status_code == 200:
            logger.info(f"âœ… ë¶„ì„ ì™„ë£Œ ë° ê²°ê³¼ ì „ì†¡ - job_id: {job_id}, ì²˜ë¦¬ì‹œê°„: {processing_time:.2f}ì´ˆ")
        else:
            logger.error(f"ê²°ê³¼ ì „ì†¡ ì‹¤íŒ¨: {response.status_code}")
        
        # Job ìƒíƒœ ì—…ë°ì´íŠ¸
        jobs[job_id] = {
            "status": "completed",
            "processing_time": processing_time,
            "completed_at": datetime.now().isoformat()
        }
        
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        try:
            if Path(video_path).exists():
                Path(video_path).unlink()
        except:
            pass
            
    except Exception as e:
        logger.error(f"ì²˜ë¦¬ ì‹¤íŒ¨ - job_id: {job_id}, error: {str(e)}")
        
        # ì—ëŸ¬ ì½œë°± ì „ì†¡
        await send_error_to_backend(
            job_id,
            str(e),
            "PROCESSING_ERROR" if "download" not in str(e).lower() else "DOWNLOAD_ERROR"
        )
        
        # Job ìƒíƒœ ì—…ë°ì´íŠ¸
        jobs[job_id] = {
            "status": "failed",
            "error": str(e),
            "failed_at": datetime.now().isoformat()
        }


# convert_to_backend_format í•¨ìˆ˜ ì œê±°ë¨ - ì´ì œ ìƒì„¸ ê²°ê³¼ë¥¼ ì§ì ‘ ë°˜í™˜


@app.post("/transcribe", response_model=BackendTranscribeResponse)
async def transcribe(request: TranscribeRequest):
    """
    ë°±ì—”ë“œ í˜¸í™˜ ë™ê¸° ì „ì‚¬ API

    Backendê°€ í˜¸ì¶œí•˜ëŠ” ë©”ì¸ API:
    POST http://localhost:8080/transcribe
    Content-Type: application/json
    """
    start_time = datetime.now()

    try:
        logger.info(f"ì „ì‚¬ ìš”ì²­ ì‹œì‘ - video_path: {request.video_path}")

        # ê°€ìƒì˜ job_id ìƒì„± (ì§„í–‰ìƒí™© ì¶”ì ìš©)
        job_id = str(uuid.uuid4())

        # Progress: íŒŒì¼ ê²€ì¦ (5%)
        await send_progress_to_backend(job_id, 5, "íŒŒì¼ ê²€ì¦ ì¤‘...")

        # ì„ì‹œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ/ê²€ì¦
        video_path = request.video_path
        if not video_path.startswith("/") and not video_path.startswith("http"):
            # S3 í‚¤ì¸ ê²½ìš° S3ì—ì„œ ë‹¤ìš´ë¡œë“œ
            video_path = request.video_path

        # Progress: ì˜¤ë””ì˜¤ ì¶”ì¶œ (15%)
        await send_progress_to_backend(job_id, 15, "ì˜¤ë””ì˜¤ ì¶”ì¶œ ì¤‘...")

        with tempfile.NamedTemporaryFile(suffix=".mp4", delete=False) as temp_file:
            try:
                # S3ì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œë„
                s3_client.download_file(S3_BUCKET, request.video_path, temp_file.name)
                logger.info(f"S3ì—ì„œ ë¹„ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {request.video_path}")
                actual_video_path = temp_file.name
            except Exception as s3_error:
                logger.warning(f"S3 ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨, ë¡œì»¬ ê²½ë¡œ ì‚¬ìš©: {s3_error}")
                actual_video_path = request.video_path

            # Progress: ìŒì„± ì¸ì‹ (25%)
            await send_progress_to_backend(job_id, 25, "ìŒì„± êµ¬ê°„ ë¶„ì„ ì¤‘...")

            # ë¶„ì„ ì„¤ì • ìƒì„±
            config = AnalysisConfig(
                enable_gpu=request.enable_gpu,
                emotion_detection=request.emotion_detection,
                language=request.language,
                max_workers=request.max_workers,
            )

            # Progress: ì „ì‚¬ ì‘ì—… (65%)
            await send_progress_to_backend(job_id, 65, "ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ ì¤‘...")

            try:
                # ì‹¤ì œ ë¶„ì„ ì‹¤í–‰
                from src.services.callback_pipeline import CallbackPipelineManager

                # Progress ì½œë°± í•¨ìˆ˜ ì •ì˜
                async def progress_callback(
                    stage: str, progress: float, message: str = ""
                ):
                    # 65% ~ 95% êµ¬ê°„ì—ì„œ ì„¸ë¶€ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                    adjusted_progress = 65 + int(progress * 30)  # 65%ì—ì„œ 95%ê¹Œì§€
                    await send_progress_to_backend(job_id, adjusted_progress, message or stage)

                pipeline = CallbackPipelineManager(config, fastapi_client=None)

                result = await pipeline._run_pipeline_with_progress(
                    input_source=actual_video_path,
                    progress_callback=progress_callback,
                    output_path=request.output_path,
                )

                # Progress: ê°ì • ë¶„ì„ (95%)
                await send_progress_to_backend(job_id, 95, "ê°ì • ë¶„ì„ ì¤‘...")

                processing_time = (datetime.now() - start_time).total_seconds()
                
                # ë””ë²„ê¹…: result ê°ì²´ êµ¬ì¡° í™•ì¸
                logger.info(f"Result ê°ì²´ íƒ€ì…: {type(result)}")
                logger.info(f"Result ì†ì„±ë“¤: {dir(result)}")
                
                # WhisperX ê²°ê³¼ ì§ì ‘ ê°€ì ¸ì˜¤ê¸°
                whisperx_result = None
                if hasattr(pipeline.pipeline_manager, '_last_whisperx_result'):
                    whisperx_result = pipeline.pipeline_manager._last_whisperx_result
                    logger.info(f"WhisperX ê²°ê³¼ ë°œê²¬: {len(whisperx_result.get('segments', []))} segments")

                # ìƒì„¸ ë¶„ì„ ê²°ê³¼ë¥¼ ì§ì ‘ ë°˜í™˜ (simplified format)
                detailed_result = {
                    "success": True,
                    "metadata": {
                        "filename": result.metadata.filename if result.metadata and hasattr(result.metadata, 'filename') else video_path,
                        "duration": result.metadata.duration if result.metadata and hasattr(result.metadata, 'duration') else 0,
                        "sample_rate": 16000,
                        "processed_at": result.metadata.analysis_timestamp if result.metadata and hasattr(result.metadata, 'analysis_timestamp') else datetime.now().isoformat(),
                        "processing_time": processing_time,
                        "total_segments": 0,  # Will be updated
                        "unique_speakers": result.metadata.total_speakers if result.metadata and hasattr(result.metadata, 'total_speakers') else 0,
                        "processing_mode": "real_ml_models",
                        "config": {
                            "enable_gpu": request.enable_gpu,
                            "segment_length": 5.0,
                            "language": request.language,
                            "unified_model": "whisperx-base-with-diarization",
                        },
                        "subtitle_optimization": True
                    },
                    "speakers": {},
                    "segments": [],
                    "processing_time": processing_time,
                }

                # WhisperX ê²°ê³¼ì—ì„œ ì§ì ‘ ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì¶œ
                if whisperx_result and "segments" in whisperx_result:
                    segments = whisperx_result["segments"]
                    logger.info(f"WhisperX ê²°ê³¼ì—ì„œ {len(segments)} ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì¶œ")
                    
                    # ìŒí–¥ íŠ¹ì„± ì¶”ì¶œ (ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°)
                    acoustic_features_list: list[AudioFeatures] = []
                    try:
                        from src.services.acoustic_analyzer import FastAcousticAnalyzer
                        
                        # ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ í™•ì¸
                        audio_path = None
                        if hasattr(pipeline.pipeline_manager, '_last_audio_path'):
                            audio_path = pipeline.pipeline_manager._last_audio_path
                        elif actual_video_path.endswith('.mp4'):
                            # ë¹„ë””ì˜¤ì—ì„œ ì¶”ì¶œëœ ì˜¤ë””ì˜¤ íŒŒì¼ ì°¾ê¸°
                            temp_audio = actual_video_path.replace('.mp4', '.wav')
                            if Path(temp_audio).exists():
                                audio_path = temp_audio
                            else:
                                # ì„ì‹œ ì˜¤ë””ì˜¤ ì¶”ì¶œ
                                import subprocess
                                temp_audio = f"/tmp/audio_{datetime.now().strftime('%Y%m%d_%H%M%S')}.wav"
                                subprocess.run([
                                    "ffmpeg", "-i", actual_video_path,
                                    "-vn", "-acodec", "pcm_s16le", "-ar", "16000",
                                    "-ac", "1", temp_audio, "-y"
                                ], capture_output=True)
                                if Path(temp_audio).exists():
                                    audio_path = temp_audio
                        
                        if audio_path and Path(audio_path).exists():
                            analyzer = FastAcousticAnalyzer(sample_rate=16000)
                            
                            # ê° ì„¸ê·¸ë¨¼íŠ¸ì— ëŒ€í•´ ìŒí–¥ íŠ¹ì„± ì¶”ì¶œ
                            for seg in segments:
                                features = analyzer.extract_features(
                                    Path(audio_path),
                                    seg.get("start", 0.0),
                                    seg.get("end", 0.0)
                                )
                                acoustic_features_list.append(features)
                            
                            logger.info(f"ìŒí–¥ íŠ¹ì„± ì¶”ì¶œ ì™„ë£Œ: {len(acoustic_features_list)}ê°œ ì„¸ê·¸ë¨¼íŠ¸")
                    except Exception as e:
                        logger.warning(f"ìŒí–¥ íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                        # ê¸°ë³¸ê°’ ì‚¬ìš© - AudioFeatures ê°ì²´ë¡œ ìƒì„±
                        from src.models.output_models import VolumeCategory
                        acoustic_features_list = [AudioFeatures(
                            rms_energy=0.05,
                            rms_db=-20.0,
                            pitch_mean=150.0,
                            pitch_variance=20.0,
                            speaking_rate=1.0,
                            amplitude_max=0.1,
                            silence_ratio=0.2,
                            spectral_centroid=1500.0,
                            zcr=0.1,
                            mfcc=[0.0, 0.0, 0.0],
                            volume_category=VolumeCategory.MEDIUM,
                            volume_peaks=[]
                        ) for _ in segments]
                    
                    # í™”ìë³„ í†µê³„ ê³„ì‚° (simplified)
                    speakers_stats = {}
                    for seg in segments:
                        speaker_id = seg.get("speaker", "SPEAKER_00")
                        if speaker_id not in speakers_stats:
                            speakers_stats[speaker_id] = {
                                "total_duration": 0.0,
                                "segment_count": 0
                            }
                        
                        duration = seg.get("end", 0.0) - seg.get("start", 0.0)
                        speakers_stats[speaker_id]["total_duration"] += duration
                        speakers_stats[speaker_id]["segment_count"] += 1
                    
                    # ê° ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ìƒì„¸ ê²°ê³¼ì— ì¶”ê°€
                    for i, seg in enumerate(segments):
                        # ìŒí–¥ íŠ¹ì„± ê°€ì ¸ì˜¤ê¸° (simplified acoustic features)
                        if i < len(acoustic_features_list):
                            af = acoustic_features_list[i]
                            # Type cast to help Pylance understand the type
                            af_typed = cast(AudioFeatures, af)
                            acoustic_features_dict = {
                                "volume_db": af_typed.rms_db,
                                "pitch_hz": af_typed.pitch_mean,
                                "spectral_centroid": af_typed.spectral_centroid,
                                "zero_crossing_rate": af_typed.zcr,
                                "pitch_mean": af_typed.pitch_mean,
                                "pitch_std": af_typed.pitch_variance ** 0.5 if af_typed.pitch_variance > 0 else 0.0,
                                "mfcc_mean": af_typed.mfcc if hasattr(af_typed, 'mfcc') and af_typed.mfcc else [0.0] * 13
                            }
                        else:
                            acoustic_features_dict = {
                                "volume_db": -20.0,
                                "pitch_hz": 150.0,
                                "spectral_centroid": 1500.0,
                                "zero_crossing_rate": 0.1,
                                "pitch_mean": 150.0,
                                "pitch_std": 20.0,
                                "mfcc_mean": [0.0] * 13
                            }
                        
                        # ì „ì²´ ì„¸ê·¸ë¨¼íŠ¸ ë°ì´í„° (simplified format without emotion/confidence)
                        segment_data = {
                            "start_time": seg.get("start", 0.0),
                            "end_time": seg.get("end", 0.0),
                            "duration": seg.get("end", 0.0) - seg.get("start", 0.0),
                            "speaker": {
                                "speaker_id": seg.get("speaker", "SPEAKER_00")
                            },
                            "acoustic_features": acoustic_features_dict,
                            "text": seg.get("text", "").strip(),
                            "words": []
                        }
                        
                        # Extract word-level information if available
                        if "words" in seg and audio_path and Path(audio_path).exists():
                            words = seg["words"]
                            for word in words:
                                word_start = word.get("start", 0.0)
                                word_end = word.get("end", 0.0)
                                
                                # Calculate acoustic features for each word
                                try:
                                    word_features = analyzer.extract_features(
                                        Path(audio_path),
                                        word_start,
                                        word_end
                                    )
                                    word_data = {
                                        "word": word.get("word", ""),
                                        "start_time": word_start,
                                        "end_time": word_end,
                                        "duration": word_end - word_start,
                                        "acoustic_features": {
                                            "volume_db": word_features.rms_db,
                                            "pitch_hz": word_features.pitch_mean,
                                            "spectral_centroid": word_features.spectral_centroid
                                        }
                                    }
                                except Exception:
                                    # Fallback values if feature extraction fails
                                    word_data = {
                                        "word": word.get("word", ""),
                                        "start_time": word_start,
                                        "end_time": word_end,
                                        "duration": word_end - word_start,
                                        "acoustic_features": {
                                            "volume_db": -20.0,
                                            "pitch_hz": 150.0,
                                            "spectral_centroid": 1500.0
                                        }
                                    }
                                
                                segment_data["words"].append(word_data)
                        
                        # segments ë°°ì—´ì— ì¶”ê°€
                        detailed_result["segments"].append(segment_data)
                    
                    # í™”ì í†µê³„ ì¶”ê°€
                    detailed_result["speakers"] = speakers_stats
                    
                    # Update total_segments in metadata
                    detailed_result["metadata"]["total_segments"] = len(detailed_result["segments"])
                else:
                    logger.warning("WhisperX ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ - timelineì—ì„œ ì¶”ì¶œ ì‹œë„")
                    
                    # timeline ì„¸ê·¸ë¨¼íŠ¸ì—ì„œ ë°ì´í„° ì¶”ì¶œ (í´ë°±)
                    if hasattr(result, "timeline") and result.timeline:
                        for seg in result.timeline:
                            # ì „ì‚¬ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ë³€í™˜ (simplified format)
                            segment_data = {
                                "start_time": seg.start_time,
                                "end_time": seg.end_time,
                                "duration": seg.end_time - seg.start_time,
                                "speaker": {
                                    "speaker_id": getattr(seg, "speaker_id", "SPEAKER_00")
                                },
                                "acoustic_features": {
                                    "volume_db": -20.0,
                                    "pitch_hz": 150.0,
                                    "spectral_centroid": 1500.0,
                                    "zero_crossing_rate": 0.1,
                                    "pitch_mean": 150.0,
                                    "pitch_std": 20.0,
                                    "mfcc_mean": [0.0] * 13
                                },
                                "text": getattr(seg, "text_placeholder", "[TRANSCRIPTION_PENDING]"),
                                "language": "ko",
                                "words": []  # No word-level data available in fallback
                            }
                            detailed_result["segments"].append(segment_data)


                # Remove performance_stats from main result (keep it minimal)
                
                # Remove optimization_stats to keep response minimal

                # Progress: ì™„ë£Œ (100%)
                await send_progress_to_backend(job_id, 100, "ë¶„ì„ ì™„ë£Œ")
                
                logger.info(f"ì „ì‚¬ ì™„ë£Œ - ì²˜ë¦¬ì‹œê°„: {processing_time:.2f}ì´ˆ")
                logger.info(f"ë°˜í™˜í•  ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜: {len(detailed_result['segments'])}")
                
                # ê²°ê³¼ë¥¼ output í´ë”ì— ì €ì¥
                output_dir = Path("/Users/ahntaeju/project/ecg-audio-analyzer/output")
                output_dir.mkdir(exist_ok=True)
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                output_file = output_dir / f"api_result_{timestamp}.json"
                
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(detailed_result, f, ensure_ascii=False, indent=2)
                logger.info(f"ê²°ê³¼ ì €ì¥ë¨: {output_file}")

                return BackendTranscribeResponse(**detailed_result)

            except Exception as analysis_error:
                logger.error(f"ë¶„ì„ ì‹¤íŒ¨: {analysis_error}")
                processing_time = (datetime.now() - start_time).total_seconds()

                return BackendTranscribeResponse(
                    success=False,
                    error=f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(analysis_error)}",
                    error_code="ANALYSIS_ERROR",
                    processing_time=processing_time,
                )

    except Exception as e:
        processing_time = (datetime.now() - start_time).total_seconds()
        logger.error(f"ì „ì‚¬ ìš”ì²­ ì‹¤íŒ¨ - Error: {str(e)}")

        return BackendTranscribeResponse(
            success=False,
            error=f"ìš”ì²­ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}",
            error_code="REQUEST_ERROR",
            processing_time=processing_time,
        )


@app.get("/health")
async def health_check():
    """ML ì„œë²„ í—¬ìŠ¤ ì²´í¬ - Backend í˜¸í™˜"""
    return {"status": "healthy", "service": "model-server"}


@app.post("/process-video")
async def process_video(request: dict, background_tasks: BackgroundTasks):
    """
    ë°±ì—”ë“œ í˜¸í™˜ ë¹„ë™ê¸° ë¹„ë””ì˜¤ ì²˜ë¦¬ ì—”ë“œí¬ì¸íŠ¸ (legacy)

    ì´ ì—”ë“œí¬ì¸íŠ¸ëŠ” ê¸°ì¡´ ë°±ì—”ë“œê°€ í˜¸ì¶œí•  ìˆ˜ ìˆë„ë¡ ìœ ì§€ë©ë‹ˆë‹¤.
    """
    try:
        # dictë¡œ ë°›ì€ ìš”ì²­ì„ TranscribeRequestë¡œ ë³€í™˜
        video_path = request.get("video_path", "")

        transcribe_request = TranscribeRequest(
            video_path=video_path,
            video_url=request.get("video_url"),
            enable_gpu=request.get("enable_gpu", True),
            emotion_detection=request.get("emotion_detection", True),
            language=request.get("language", "auto"),
            max_workers=request.get("max_workers", 4),
            output_path=request.get("output_path"),
        )

        # /transcribe ì—”ë“œí¬ì¸íŠ¸ì™€ ë™ì¼í•œ ì²˜ë¦¬
        return await transcribe(transcribe_request)

    except Exception as e:
        logger.error(f"Process video ìš”ì²­ ì‹¤íŒ¨: {str(e)}")
        return BackendTranscribeResponse(
            success=False,
            error=f"ìš”ì²­ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}",
            error_code="REQUEST_ERROR",
            processing_time=0.0,
        )


@app.post("/process-video-legacy")
async def process_video_legacy(video_key: str, background_tasks: BackgroundTasks):
    """
    ë ˆê±°ì‹œ ë™ê¸° ì—”ë“œí¬ì¸íŠ¸ (í˜¸í™˜ì„± ìœ ì§€)
    """
    return await request_process(video_key, background_tasks)


@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "service": "ECG Audio Analyzer ML API",
        "version": "1.0.0",
        "description": "EC2 ML ì„œë²„ - ë¹„ë””ì˜¤ ì˜¤ë””ì˜¤ ë¶„ì„ API",
        "endpoints": {
            "process-video-api": "POST /api/upload-video/process-video - ë©”ì¸ ë¹„ë””ì˜¤ ì²˜ë¦¬ API (ëª…ì„¸ ì¤€ìˆ˜)",
            "transcribe": "POST /transcribe - ë™ê¸° ì „ì‚¬ API",
            "request-process": "POST /request-process - ë¹„ë™ê¸° ë¹„ë””ì˜¤ ë¶„ì„ (legacy)",
            "process-video": "POST /process-video - ë ˆê±°ì‹œ ë¹„ë””ì˜¤ ë¶„ì„",
            "health": "GET /health - í—¬ìŠ¤ ì²´í¬",
            "docs": "GET /docs - API ë¬¸ì„œ",
        },
        "backend_url": BACKEND_URL
    }


@app.post("/test")
async def test_endpoint():
    """í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "status": "ok",
        "message": "ML API ì„œë²„ê°€ ì •ìƒ ì‘ë™ì¤‘ì…ë‹ˆë‹¤",
        "timestamp": datetime.now().isoformat(),
    }


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="ECG Audio Analyzer ML API Server")
    parser.add_argument("--host", default="0.0.0.0", help="ë°”ì¸ë“œ í˜¸ìŠ¤íŠ¸")
    parser.add_argument("--port", type=int, default=8080, help="ë°”ì¸ë“œ í¬íŠ¸")

    parser.add_argument("--workers", type=int, default=1, help="ì›Œì»¤ ìˆ˜")
    parser.add_argument(
        "--log-level", default="info", choices=["debug", "info", "warning", "error"]
    )

    args = parser.parse_args()

    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=getattr(logging, args.log_level.upper()),
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    )

    logger.info("ğŸš€ ECG Audio Analyzer ML API ì„œë²„ ì‹œì‘")
    logger.info(f"   í˜¸ìŠ¤íŠ¸: {args.host}:{args.port}")
    logger.info(f"   ì›Œì»¤ ìˆ˜: {args.workers}")
    logger.info(f"   ë¡œê·¸ ë ˆë²¨: {args.log_level}")
    logger.info(f"   ë°±ì—”ë“œ URL: {BACKEND_URL}")

    # GPU í™•ì¸
    try:
        import torch

        if torch.cuda.is_available():
            logger.info(f"   GPU: {torch.cuda.device_count()}ê°œ ì‚¬ìš© ê°€ëŠ¥")
        else:
            logger.warning("   GPU: ì‚¬ìš© ë¶ˆê°€ (CPU ëª¨ë“œ)")
    except ImportError:
        logger.warning("   PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")

    # FastAPI ì„œë²„ ì‹¤í–‰
    uvicorn.run(
        app,
        host=args.host,
        port=args.port,
        workers=args.workers,
        access_log=True,
        log_level=args.log_level,
    )
