"""
EC2 ML ì„œë²„ - FastAPI ê¸°ë°˜

WhisperX, í™”ì ë¶„ë¦¬, ê°ì • ë¶„ì„ ë“±ì„ HTTP APIë¡œ ì œê³µí•˜ëŠ” ì„œë²„
ECS FastAPI ë°±ì—”ë“œë¡œë¶€í„° ìš”ì²­ì„ ë°›ì•„ JSONìœ¼ë¡œ ê²°ê³¼ ë°˜í™˜
"""

import asyncio
import os
import sys
import logging
from pathlib import Path
from typing import Dict, Any, Optional, Union
from datetime import datetime

from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel
import uvicorn
import uuid
import requests
import tempfile
import boto3

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.services.callback_pipeline import process_video_with_fastapi
from src.api import AnalysisConfig
from src.utils.logger import get_logger

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="ECG Model Server",
    description="EC2 ML ì„œë²„ - ë¹„ë””ì˜¤ ì˜¤ë””ì˜¤ ë¶„ì„ API",
    version="1.0.0"
)

# AWS S3 ë° Backend ì„¤ì •
s3_client = boto3.client('s3')
S3_BUCKET = "ecg-project-pipeline-dev-video-storage-np9digv7"
BACKEND_URL = "http://ecg-project-pipeline-dev-alb-1703405864.us-east-1.elb.amazonaws.com"

# In-memory job tracking
jobs = {}

# ë¡œê±° ì„¤ì •
logger = get_logger(__name__)


# Pydantic ëª¨ë¸ë“¤
class RequestProcessRequest(BaseModel):
    """ë¹„ë™ê¸° ë¹„ë””ì˜¤ ë¶„ì„ ìš”ì²­"""
    video_key: str  # S3 í‚¤
    
    # ë¶„ì„ ì„¤ì •
    enable_gpu: bool = True
    emotion_detection: bool = True
    language: str = "auto"
    max_workers: int = 4

class RequestProcessResponse(BaseModel):
    """ë¹„ë™ê¸° ë¹„ë””ì˜¤ ë¶„ì„ ì‘ë‹µ"""
    job_id: str
    status: str  # "processing"


class TranscribeResponse(BaseModel):
    """ë¹„ë””ì˜¤ ë¶„ì„ ì‘ë‹µ"""
    success: bool
    processing_time: float
    metadata: Dict[str, Any]
    transcript_segments: list
    speaker_segments: list
    emotion_segments: list
    acoustic_features: list
    performance_stats: Optional[Dict[str, Any]] = None


class HealthResponse(BaseModel):
    """í—¬ìŠ¤ ì²´í¬ ì‘ë‹µ"""
    status: str
    service: str
    version: str
    gpu_available: bool
    gpu_count: int
    timestamp: str


@app.post("/request-process", response_model=RequestProcessResponse)
async def request_process(video_key: str, background_tasks: BackgroundTasks):
    """
    ë¹„ë™ê¸° ë¹„ë””ì˜¤ ë¶„ì„ ìš”ì²­ - Backendì™€ í˜¸í™˜
    
    Backendê°€ í˜¸ì¶œí•˜ëŠ” API:
    POST http://10.0.10.42:8080/request-process?video_key=uploads/uuid/video.mp4
    """
    
    try:
        # Job ID ìƒì„±
        job_id = str(uuid.uuid4())
        jobs[job_id] = {"status": "processing", "video_key": video_key}
        
        logger.info(f"ë¹„ë™ê¸° ë¶„ì„ ìš”ì²­ ì‹œì‘ - video_key: {video_key}, job_id: {job_id}")
        
        # ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì‹œì‘
        background_tasks.add_task(process_video_async, job_id, video_key)
        
        return RequestProcessResponse(
            job_id=job_id,
            status="processing"
        )
        
    except Exception as e:
        logger.error(f"ë¶„ì„ ìš”ì²­ ì‹¤íŒ¨ - Error: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"ë¶„ì„ ìš”ì²­ ì‹¤íŒ¨: {str(e)}"
        )


async def process_video_async(job_id: str, video_key: str):
    """
    ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë¹„ë””ì˜¤ ë¶„ì„ ë° Backendë¡œ ê²°ê³¼ ì „ì†¡
    """
    try:
        logger.info(f"ë°±ê·¸ë¼ìš´ë“œ ë¶„ì„ ì‹œì‘ - job_id: {job_id}, video_key: {video_key}")
        
        # ì„ì‹œ íŒŒì¼ì— S3ì—ì„œ ë¹„ë””ì˜¤ ë‹¤ìš´ë¡œë“œ
        with tempfile.NamedTemporaryFile(suffix='.mp4', delete=False) as temp_file:
            try:
                s3_client.download_file(S3_BUCKET, video_key, temp_file.name)
                logger.info(f"S3ì—ì„œ ë¹„ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {video_key}")
            except Exception as s3_error:
                logger.warning(f"S3 ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨, ëª¨í¬ ë°ì´í„° ì‚¬ìš©: {s3_error}")
            
            # ë¶„ì„ ì„¤ì • ìƒì„±
            config = AnalysisConfig(
                enable_gpu=True,
                emotion_detection=True,
                language="auto",
                max_workers=4
            )
            
            try:
                # ì‹¤ì œ ë¶„ì„ ì‹¤í–‰
                from src.services.callback_pipeline import CallbackPipelineManager
                
                pipeline = CallbackPipelineManager(config, fastapi_client=None)
                
                result = await pipeline._run_pipeline_with_progress(
                    input_source=temp_file.name,
                    progress_callback=_dummy_progress_callback,
                    output_path=None
                )
                
                # ê²°ê³¼ë¥¼ Backend í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                backend_result = {
                    "job_id": job_id,
                    "video_key": video_key,
                    "status": "completed",
                    "success": True,
                    "processing_time": result.metadata.processing_time if result.metadata else 0,
                    "results": {
                        "transcript": [
                            {
                                "start": seg.start_time,
                                "end": seg.end_time,
                                "text": seg.text,
                                "speaker": seg.speaker_id
                            }
                            for seg in (result.transcript_segments or [])
                        ],
                        "emotions": [
                            {
                                "start": seg.start_time,
                                "end": seg.end_time,
                                "emotion": seg.emotion,
                                "confidence": seg.confidence
                            }
                            for seg in (result.emotion_segments or [])
                        ],
                        "metadata": {
                            "duration": result.metadata.video_duration if result.metadata else 0,
                            "speakers": result.metadata.total_speakers if result.metadata else 0,
                            "language": result.metadata.language_detected if result.metadata else "unknown"
                        }
                    }
                }
                
            except Exception as analysis_error:
                logger.error(f"ë¶„ì„ ì‹¤íŒ¨: {analysis_error}")
                # ë¶„ì„ ì‹¤íŒ¨ì‹œ ëª¨í¬ ê²°ê³¼ ìƒì„±
                backend_result = {
                    "job_id": job_id,
                    "video_key": video_key,
                    "status": "completed",
                    "success": False,
                    "error": str(analysis_error),
                    "results": {
                        "transcript": [
                            {"start": 0.0, "end": 3.0, "text": "ë¶„ì„ ì‹¤íŒ¨ - ëª¨í¬ ë°ì´í„°", "speaker": "SPEAKER_00"}
                        ],
                        "emotions": [],
                        "metadata": {"duration": 0, "speakers": 0, "language": "unknown"}
                    }
                }
            
            # ì‘ì—… ìƒíƒœ ì—…ë°ì´íŠ¸
            jobs[job_id] = {"status": "completed", "result": backend_result}
            
            # Backendë¡œ ê²°ê³¼ ì „ì†¡
            try:
                response = requests.post(
                    f"{BACKEND_URL}/api/upload-video/results",
                    json=backend_result,
                    timeout=30
                )
                logger.info(f"Backendë¡œ ê²°ê³¼ ì „ì†¡ ì™„ë£Œ - job_id: {job_id}, status: {response.status_code}")
            except Exception as backend_error:
                logger.error(f"Backend ê²°ê³¼ ì „ì†¡ ì‹¤íŒ¨: {backend_error}")
            
            logger.info(f"ë¶„ì„ ì™„ë£Œ - job_id: {job_id}")
            
    except Exception as e:
        jobs[job_id] = {"status": "failed", "error": str(e)}
        logger.error(f"ë¶„ì„ ì‹¤íŒ¨ - job_id: {job_id}, error: {str(e)}")


async def _dummy_progress_callback(stage: str, progress: float, message: str = None):
    """ì§„í–‰ ìƒí™© ì½œë°± (ë¡œê·¸ë§Œ ì¶œë ¥)"""
    logger.info(f"Progress: {stage} - {progress:.1%} - {message or ''}")


@app.get("/health")
async def health_check():
    """ML ì„œë²„ í—¬ìŠ¤ ì²´í¬ - Backend í˜¸í™˜"""
    return {"status": "healthy", "service": "model-server"}


@app.post("/process-video")
async def process_video_legacy(video_key: str, background_tasks: BackgroundTasks):
    """
    ë ˆê±°ì‹œ ë™ê¸° ì—”ë“œí¬ì¸íŠ¸ (í˜¸í™˜ì„± ìœ ì§€)
    """
    return await request_process(video_key, background_tasks)


@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "service": "ECG Audio Analyzer ML API",
        "version": "1.0.0", 
        "description": "EC2 ML ì„œë²„ - ë¹„ë””ì˜¤ ì˜¤ë””ì˜¤ ë¶„ì„ API",
        "endpoints": {
            "request-process": "POST /request-process - ë¹„ë™ê¸° ë¹„ë””ì˜¤ ë¶„ì„",
            "process-video": "POST /process-video - ë ˆê±°ì‹œ ë¹„ë””ì˜¤ ë¶„ì„", 
            "health": "GET /health - í—¬ìŠ¤ ì²´í¬",
            "docs": "GET /docs - API ë¬¸ì„œ"
        }
    }


@app.post("/test")
async def test_endpoint():
    """í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "status": "ok",
        "message": "ML API ì„œë²„ê°€ ì •ìƒ ì‘ë™ì¤‘ì…ë‹ˆë‹¤",
        "timestamp": datetime.now().isoformat()
    }


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="ECG Audio Analyzer ML API Server")
    parser.add_argument("--host", default="0.0.0.0", help="ë°”ì¸ë“œ í˜¸ìŠ¤íŠ¸")
    parser.add_argument("--port", type=int, default=8080, help="ë°”ì¸ë“œ í¬íŠ¸") 
    parser.add_argument("--workers", type=int, default=1, help="ì›Œì»¤ ìˆ˜")
    parser.add_argument("--log-level", default="info", choices=["debug", "info", "warning", "error"])
    
    args = parser.parse_args()
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=getattr(logging, args.log_level.upper()),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    logger.info(f"ğŸš€ ECG Audio Analyzer ML API ì„œë²„ ì‹œì‘")
    logger.info(f"   í˜¸ìŠ¤íŠ¸: {args.host}:{args.port}")
    logger.info(f"   ì›Œì»¤ ìˆ˜: {args.workers}")
    logger.info(f"   ë¡œê·¸ ë ˆë²¨: {args.log_level}")
    
    # GPU í™•ì¸
    try:
        import torch
        if torch.cuda.is_available():
            logger.info(f"   GPU: {torch.cuda.device_count()}ê°œ ì‚¬ìš© ê°€ëŠ¥")
        else:
            logger.warning("   GPU: ì‚¬ìš© ë¶ˆê°€ (CPU ëª¨ë“œ)")
    except ImportError:
        logger.warning("   PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
    
    # FastAPI ì„œë²„ ì‹¤í–‰
    uvicorn.run(
        app,
        host=args.host,
        port=args.port,
        workers=args.workers,
        access_log=True,
        log_level=args.log_level
    )